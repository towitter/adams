{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Word2Vec\n",
    "The notebooks sheds light on some of the details of word2vec by showcasing how you can 're-implement' the w2v model using basic keras functionality. In case you would like to take it one step further and code everyting yourself using just numpy, I recommend [Nathan Rooy's post](https://nathanrooy.github.io/posts/2018-03-22/word2vec-from-scratch-with-python-and-numpy/) the codes of which are available from his [GitHub repo](https://github.com/nathanrooy/word2vec-from-scratch-with-python/blob/master/word2vec.py). A re-implementation of this example with a nice Excel demo is available on [Towards Data Science](https://towardsdatascience.com/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281); [here is yet another example.](https://towardsdatascience.com/word2vec-from-scratch-with-numpy-8786ddd49e72) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['_unit_id', '_golden', '_unit_state', '_trusted_judgments',\n",
      "       '_last_judgment_at', 'audience', 'audience:confidence', 'bias',\n",
      "       'bias:confidence', 'message', 'message:confidence', 'orig__golden',\n",
      "       'audience_gold', 'bias_gold', 'bioid', 'embed', 'id', 'label',\n",
      "       'message_gold', 'source', 'text'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn import model_selection\n",
    "import nltk\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import gensim\n",
    "import collections\n",
    "\n",
    "#Setting dataframe max limit of columns in output to 10\n",
    "pd.set_option('display.max_columns', 10)\n",
    "\n",
    "#Loading data and printing the columns\n",
    "data = pd.read_csv(\"political_social_media.csv\", encoding = \"ISO-8859-1\")\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[[\"message\",\"message:confidence\",\"label\",\"source\",\"text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems that we can trust the labels. Let's now look at the text:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As POTUS golfs, pushes amnesty &amp; ignores Keystone, American people are concerned about jobs, econ &amp; health care costs http://t.co/p9sPDYOAca'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = list(data.text)\n",
    "raw[10]\n",
    "#ok, we will probably have to clean it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['as',\n",
       " 'potus',\n",
       " 'golfs',\n",
       " 'pushes',\n",
       " 'amnesty',\n",
       " 'amp',\n",
       " 'ignores',\n",
       " 'keystone',\n",
       " 'american',\n",
       " 'people',\n",
       " 'are',\n",
       " 'concerned',\n",
       " 'about',\n",
       " 'jobs',\n",
       " 'econ',\n",
       " 'amp',\n",
       " 'health',\n",
       " 'care',\n",
       " 'costs']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word2vec expexts a list of list: each document is a list of tokens\n",
    "import re\n",
    "prep=[]\n",
    "for i,line in enumerate(raw):\n",
    "    prep.append(gensim.utils.simple_preprocess(re.sub(r'http\\S+', '', line)))\n",
    "prep[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Text to Integer Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `keras` implementation of negative sampling uses a heuristic to determine the frequency of words: The rank of the words sorted by frequency. Thus, we want to construct a dictionary from words to integers, where the words/integers are sorted from most frequent to least frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the words and update a counter keeping track of word counts\n",
    "word_counter = collections.Counter()\n",
    "for sentence in prep:\n",
    "    for word in sentence:\n",
    "        word_counter.update({word: 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select only the most common words. Less than 6,000 words occur more than once in this dataset of tweets! The most common 3,000 words occur about 5 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = word_counter.most_common(3000)\n",
    "vocab = [x[0] for x in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab = list(set.union(*map(set, prep))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = range(1,len(vocab)+1)\n",
    "word2idx = dict(zip(vocab, idx))\n",
    "idx2word = dict(zip(idx, vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map unknown words to a special idx. This is how we deal with uncommon words or new words that come in during day-to-day business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx[\"UNKNOWN\"] = 0\n",
    "idx2word[0] = \"UNKNOWN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to help map unknown words to index 'unknown'\n",
    "def words_to_labels(sentence, dictionary):\n",
    "    output = []\n",
    "    for word in sentence:\n",
    "        if word not in dictionary.keys():\n",
    "            output.append(dictionary[\"UNKNOWN\"])\n",
    "        else:\n",
    "            output.append(dictionary[word])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Little test of our dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "627"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx[\"potus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'potus'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word[627]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn text into labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [] \n",
    "for sentence in prep:\n",
    "    corpus.append(words_to_labels(sentence, word2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[79, 0, 206, 0, 0, 1312, 0, 68, 1004],\n",
       " [332, 68, 232, 4, 407, 369, 3, 745, 1368]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/hauptjoh/anaconda/envs/adams/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Embedding, Input, Reshape, Dot, Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://adventuresinmachinelearning.com/word2vec-keras-tutorial/ has more explanations, but they use a shared embedding layer, which is not correct IMHO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding dimension.Typically 50-300 for words.\n",
    "EMB_DIM = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up embedding layers\n",
    "# Embed the target word\n",
    "embedding_target = Embedding(VOCAB_SIZE, EMB_DIM, input_length=1, name='embedding_target')\n",
    "# Embed the context word\n",
    "# As in matrix factorization, we factorize into two matrices: target embeddings and context embeddings\n",
    "embedding_context = Embedding(VOCAB_SIZE, EMB_DIM, input_length=1, name='embedding_context')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reshaping below is necessary to make the output dimensions of each layer comfort to the next layer. Check the `model.summary()` when you build functional models yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model architecture\n",
    "\n",
    "# Take a single target word\n",
    "input_target = Input((1,))\n",
    "target = embedding_target(input_target)\n",
    "target = Reshape((EMB_DIM, 1))(target)\n",
    "\n",
    "# Take another word either from the context or a random word from vocabulary\n",
    "input_context = Input((1,))\n",
    "context = embedding_context(input_context)\n",
    "context = Reshape((EMB_DIM, 1))(context)\n",
    "\n",
    "# Calculate the dot product as an unnormalized cosine distance\n",
    "dot_product = keras.layers.Dot(axes=1, normalize=False)([target, context])\n",
    "dot_product = Reshape((1,))(dot_product)\n",
    "\n",
    "# Predict if the words are in the same context -> Binary yes/no\n",
    "output = Activation(activation='sigmoid')(dot_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model = Model(inputs=[input_target, input_context], outputs=output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the model is not much of a neural network? The only trainable parameters are the embeddings, which are then dot-multiplied. We thus have two hidden layers side-by-side rather than one after the other and no non-linear activation of the hidden layers! This is very similar to matrix factorization and you can use the same architecture to build a collaborative filter on users (one embedding matrix) and items (one embedding matrix). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_target (Embedding)    (None, 1, 10)        30010       input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_context (Embedding)   (None, 1, 10)        30010       input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 10, 1)        0           embedding_target[2][0]           \n",
      "__________________________________________________________________________________________________\n",
      "reshape_8 (Reshape)             (None, 10, 1)        0           embedding_context[2][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dot_3 (Dot)                     (None, 1, 1)         0           reshape_7[0][0]                  \n",
      "                                                                 reshape_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)             (None, 1)            0           dot_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 1)            0           reshape_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 60,020\n",
      "Trainable params: 60,020\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A window size of `i` translates to $[w_{-i},\\ldots, w, \\ldots, w_{+i}]$, so the number of context words to consider is window size $\\times 2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample words with equalized probability, here a more complicated version of 1/frequency(word)\n",
    "# make_sampling_table approximates frequency by the rank of occurence frequency in the vocabulary list\n",
    "sampling_table = sequence.make_sampling_table(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sampling_table` is a list of sampling probabilities, one for each word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00315225, 0.00315225, 0.00547597, ..., 0.50726163, 0.50735608,\n",
       "       0.50745052])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`skipgrams` takes a sentence and outputs:\n",
    "\n",
    "1. target words in combination with a context word\n",
    "2. a label if the context word is from the actual context or randomly sampled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example for, importantly, a *single* sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[332, 68, 232, 4, 407, 369, 3, 745, 1368]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence = corpus[1]\n",
    "example_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[1368, 1700], [1368, 802], [1368, 3], [1368, 745]], [0, 0, 1, 1])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence.skipgrams(example_sentence, \n",
    "                   VOCAB_SIZE, window_size=WINDOW_SIZE, \n",
    "                   negative_samples=1.,  # Ratio of positive to negative samples\n",
    "                   sampling_table=sampling_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_EPOCH = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A batch should be sampled from single sentence and we update the model after one batch/sentence. In other words, the standard keras training loop doesn't fit and we train manually to make sure to update the model sentence by sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Epoch 0\n",
      "----------------------------------------\n",
      "Average loss over last 1000 batches: 0.4232877790927887\n",
      "----------------------------------------\n",
      "Epoch 1\n",
      "----------------------------------------\n",
      "Average loss over last 1000 batches: 0.4340161085128784\n",
      "----------------------------------------\n",
      "Epoch 2\n",
      "----------------------------------------\n",
      "Average loss over last 1000 batches: 0.4258624017238617\n",
      "----------------------------------------\n",
      "Epoch 3\n",
      "----------------------------------------\n",
      "Average loss over last 1000 batches: 0.4115869402885437\n",
      "----------------------------------------\n",
      "Epoch 4\n",
      "----------------------------------------\n",
      "Average loss over last 1000 batches: 0.4147600829601288\n",
      "----------------------------------------\n",
      "Epoch 5\n",
      "----------------------------------------\n",
      "Average loss over last 1000 batches: 0.41622471809387207\n",
      "----------------------------------------\n",
      "Epoch 6\n",
      "----------------------------------------\n",
      "Average loss over last 1000 batches: 0.4157622158527374\n",
      "----------------------------------------\n",
      "Epoch 7\n",
      "----------------------------------------\n",
      "Average loss over last 1000 batches: 0.4119722843170166\n",
      "----------------------------------------\n",
      "Epoch 8\n",
      "----------------------------------------\n",
      "Average loss over last 1000 batches: 0.4057624638080597\n",
      "----------------------------------------\n",
      "Epoch 9\n",
      "----------------------------------------\n",
      "Average loss over last 1000 batches: 0.403744637966156\n"
     ]
    }
   ],
   "source": [
    "for e in range(NO_EPOCH):\n",
    "        print('-'*40)\n",
    "        print('Epoch', e)\n",
    "        print('-'*40)\n",
    "\n",
    "        samples_seen = 0\n",
    "        losses = []\n",
    "        \n",
    "        for i, seq in enumerate(corpus):\n",
    "            # get skipgram couples for one text in the dataset\n",
    "            couples, labels = sequence.skipgrams(seq, VOCAB_SIZE, window_size= WINDOW_SIZE, negative_samples=1., sampling_table=sampling_table)\n",
    "            if couples:\n",
    "                # one gradient update per sentence (one sentence = a few 1000s of word couples)\n",
    "                X = np.array(couples, dtype=\"int32\")\n",
    "                loss = model.train_on_batch([X[:,0],X[:,1]], labels)\n",
    "                losses.append(loss)\n",
    "        print(f'Average loss over last 1000 batches: {np.mean(losses[-1000:])}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
