{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf400
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww22540\viewh16220\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \
- Logit Baseline without pre-processing not a good example. Do tokenization and lemmatization before building any of the models (after dictionary approach)\
\
- Train the GRU model without word2vec first to show that this is the \'91typical\'92 model. Then introduce word2vec and transfer learning. Don\'92t mix Glove and word2vec in the exercise. Rather use pretrained word2vec embeddings\
\
- Create pictures for transfer learning and bidirectional\
}