{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #3: neural networks primer (part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1 : set it up ##\n",
    "\n",
    " 1. use case\n",
    " 2. from logit to neural network\n",
    " 3. nn structure: weights and biases\n",
    " 3. activation function\n",
    " 4. softmax\n",
    " 5. application\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2 : make it work ##\n",
    " 1. loss function\n",
    " 2. gradient\n",
    " 3. weight update\n",
    " 4. learning rate\n",
    " 5. stochastic gradient descent backpropagation\n",
    " 6. application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: set it up # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 use case: rating in appstore ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "size_bytes                       float64\n",
       "price                            float64\n",
       "rating_count_tot                 float64\n",
       "sup_devices.num                  float64\n",
       "ipadSc_urls.num                  float64\n",
       "lang.num                         float64\n",
       "vpp_lic                          float64\n",
       "currency_USD                       int64\n",
       "cont_rating_12+                    int64\n",
       "cont_rating_17+                    int64\n",
       "cont_rating_4+                     int64\n",
       "cont_rating_9+                     int64\n",
       "prime_genre_Book                   int64\n",
       "prime_genre_Business               int64\n",
       "prime_genre_Catalogs               int64\n",
       "prime_genre_Education              int64\n",
       "prime_genre_Entertainment          int64\n",
       "prime_genre_Finance                int64\n",
       "prime_genre_Food & Drink           int64\n",
       "prime_genre_Games                  int64\n",
       "prime_genre_Health & Fitness       int64\n",
       "prime_genre_Lifestyle              int64\n",
       "prime_genre_Medical                int64\n",
       "prime_genre_Music                  int64\n",
       "prime_genre_Navigation             int64\n",
       "prime_genre_News                   int64\n",
       "prime_genre_Photo & Video          int64\n",
       "prime_genre_Productivity           int64\n",
       "prime_genre_Reference              int64\n",
       "prime_genre_Shopping               int64\n",
       "prime_genre_Social Networking      int64\n",
       "prime_genre_Sports                 int64\n",
       "prime_genre_Travel                 int64\n",
       "prime_genre_Utilities              int64\n",
       "prime_genre_Weather                int64\n",
       "user_rating                      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "app = pd.read_csv(\"../../data/AppleStore_prep.csv\",index_col=False,sep='\\t', encoding='utf-8')\n",
    "app=app.set_index('track_name')\n",
    "app.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7197, 36)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.shape #35 variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 from logit to neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running a classification task, we are looking for a probability of a certain outcome. Here is the sigmoid function where *p* stands for target probability:\n",
    "$$p=\\frac{1}{1+e^{-y}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression formula would then be:\n",
    "\n",
    "$$y=ln(\\frac{p}{1-p})=\\beta_{0}+\\beta_{1}X_{1}+\\beta_{2}X_{2}+...+\\beta_{k}X_{k}+e$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix notation will look like:\n",
    "\n",
    "$$\\begin{bmatrix}  y_{1}\\\\  y_{2}\\\\   \\vdots\\\\ y_{n}\\\\ \\end{bmatrix}=\\begin{bmatrix} 1 & x_{11}&\\ldots\\ & x_{1,k}\\\\  1 & x_2\\\\  \\vdots & \\vdots\\\\  1 & x_n&\\ldots\\ & x_{n,k}\\\\ \\end{bmatrix}\\begin{bmatrix}  \\beta_{0}\\\\  \\beta_{1}\\\\   \\vdots\\\\ \\beta_{k}\\\\ \\end{bmatrix} +\\begin{bmatrix}  e_{1}\\\\  e_{2}\\\\   \\vdots\\\\ e_{n} \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression actually represents a one-layer neural network:\n",
    "\n",
    "<img src=\"LogtNN.png\" alt=\"logit\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's present the formula in a slightly more general way:\n",
    "\n",
    "$$y=WX+b$$\n",
    "\n",
    "where the W is standing for the **weight** matrix, containing coefficients/betas for respective explanatory variables, and b is an error term or a **bias**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"nnn.png\" alt=\"nn\" style=\"width: 1200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So have 35 variables and a target lable with 6 levels. How do we make it a NN task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1.3 nn architecture ## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with so called \"vanilla neural network\", or ANN.\n",
    "This is not deep learning yet, as we have just one \"hidden layer\".\n",
    " - a - input layer (number of neurons defined by number of features)\n",
    " - a' - hidden layer (number of neurons depends on the task, see best practice)\n",
    " - a'' - output layer (number of neurons depends on the type of output, 1 for regression, 2 for binary logistic regression, multiclass by number of classes)\n",
    "\n",
    "<img src=\"nn_full.png\" alt=\"fff\" style=\"width: 800px;\">\n",
    "\n",
    "\n",
    "Additionally, every neuron of a hidden layer has an **activation function** that transforms the input. \n",
    "For example, if the activation function is $f(x)=x^2$ then with input $x=2$, the neuron will take the value of 4.\n",
    "\n",
    "However, the usual \"go to\" functions for neural networks in recent years have been either *sigmoid* or *ReLu*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 activation function ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deZRcdZ338fe3qrfsW3f2DklIgIQ1oYEoI1sghMgEUMSgjogO0ZlhRh+deR4cHeSgM0fxPM6Mz+AooygqEFAHzUBYIosssiSYfSNNSNKdrTtbJ51eq+r7/FGVpmiqk+6k696q6s/rnDp9l19Vf/vW7frU/d3N3B0RERGASNgFiIhI7lAoiIhIB4WCiIh0UCiIiEgHhYKIiHQoCruAk1FeXu4TJ04MuwwRkbzy5ptv7nX3ikzz8joUJk6cyPLly8MuQ0Qkr5jZtq7mqftIREQ6KBRERKSDQkFERDooFEREpINCQUREOgQSCmZ2v5nVmdnaLuabmX3fzKrNbLWZzQyiLhERea+gthR+Bsw9xvxrgKmpx0LgPwOoSUREOgnkPAV3f9HMJh6jyXXAzz15He/XzGyomY1x911B1Cci+cndiSWc1liCtliC1lic9pjTFo/TFnNiiQTtcScWTxBPOO0JJ55IEE/w7k93Egkn4U484bhDwp1E6qe/Zzj5M/m7U9NSwwDJsXfHj9b47vz3t+3c/j1/33v/2PfMmz1tFOdWDj2xBXcMuXLy2jigJm28NjXtfaFgZgtJbk0wYcKEQIoTkeyIxRPsO9LG3sZW9h9pY/+RNg4caaOhOUZDczuHW9ppbI1xuCVGY2uM5rY4Te3Jn81tcVpiyQ/7vsLs3eGRg8sKOhQsw7SM77S73wfcB1BVVdV31gaRPBRPODsPNrNl7xHeqW+k5kAzOw40s+NgM7sPtbCvsZWuPtMHlEQZ0q+YgWVFDCwtYlBZEaMGlzKgpIiykij9ipOP0qIIpcURSouiFEcjlBQlH8URozgaoSia+hkxiqJGNBIhakY0cvQBETMiqWmRiGFANGKYgZGcbiQ/lM2OTk8+72gb6/QpdnT6u8NHp1vacHr7TB+DwcuVUKgFKtPGxwM7Q6pFRE5AayzO2h2HWFVzkA27DrFx92He2nOY1liio01ZcYRxQ/sxblh/po8ZzKjBpVQMLqNiYAnDB5QyfEAJw/oXM7hfMcVRHRwZhlwJhcXA7Wa2CLgIaND+BJHc1hqL8+a2A7y8eS+vbtnHuh2HaIsnA6B8YCnTxgziL2adwpSRA5lUPoBJFQOoGFiaM9+IJbNAQsHMHgYuA8rNrBb4BlAM4O4/BJYA84BqoAm4NYi6RKRnDrW089yGOpas2cWLm+tpaU8QjRjnVQ7l1osnMmPCMGZOGMrIwWVhlyonKKijj24+znwH/iaIWkSkZxIJ56XqvTyybDu/X19HWzzB6MFl3FRVySVTK7ho8nAGlRWHXab0klzpPhKRHNPYGuPB17bx81e3seNgM8P6F/OpWafw4XPGMKNyKJGIuoEKkUJBRN7jYFMb97+ylQf+uJWG5nZmTR7OHdecwZwzR1FaFA27PMkyhYKIANAeT/DL17bxb7/fTENzO3Omj+KvL5/CeVk4Fl5yl0JBRHhpcz3fWLyOLfVH+LMp5Xztw9OYNmZw2GVJCBQKIn1YU1uMf1mygV++tp1J5QP4yS1VXHHGSB022ocpFET6qDe3HeDLj65k+/4m/vLPJvH3V59OWbH2GfR1CgWRPujhN7Zz5+/WMmpwGQ/fNotZk0eEXZLkCIWCSB/SHk/wrcfX88Cr27jktAr+380zGNJP5xjIuxQKIn1EU1uMz//iTV7avJfbPjSJO66ZRlTnGkgnCgWRPqCxNcZnf7qM5dv2c89Hz+GmCyqP/yTpkxQKIgWuobmdz/z0DVbXNvD9m2dw7Tljwy5JcphCQaSAHWmN8emfvM76XYf4wSdncvWZo8MuSXKcQkGkQMXiCf724RWs2dHAj/6iiqumjwq7JMkDCgWRAuTufGPxOp7bWMc/33CWAkG6Tbc2EilAP3pxCw++vp0vXHoqn7zolLDLkTyiUBApMC9truc7T23k2nPG8L+vPj3sciTPKBRECkjdoRb+1yMrmVIxkO/eeK7ueSA9pn0KIgUinnC+uGglja0xHrptFv1KdB0j6TmFgkiB+I/nqnl1yz7u+eg5nDZqUNjlSJ5S95FIAVhZc5B/f/Ytrj9vLB+rGh92OZLHFAoiea49nuCO36ymYlApd19/lu6FICdF3Uciee6+F7ewcfdh7vuL8xlcpiueysnRloJIHttS38i/P7uZeWePZo4uYSG9QKEgkqfcna/+9xrKiiLcNf/MsMuRAqFQEMlTi1ft5PV39vOP86YxclBZ2OVIgVAoiOShlvY49zy1ieljBnNTle6NIL1HoSCSh372x63sONjM1z88TWctS69SKIjkmX2Nrdz7XDWzzxjJB6eUh12OFBiFgkie+f6zm2lqj/PVeWeEXYoUIIWCSB55Z+8RHnx9OwsuqGTKSF3KQnqfQkEkj9z7fDXRiPHFK6eGXYoUqMBCwczmmtkmM6s2szsyzJ9gZs+b2QozW21m84KqTSQf1Oxv4rEVO/jERRN0CKpkTSChYGZR4F7gGmA6cLOZTe/U7OvAo+4+A1gA/CCI2kTyxQ9eqCZqxucvOTXsUqSABbWlcCFQ7e5b3L0NWARc16mNA4NTw0OAnQHVJpLzdhxs5tdv1nLTBeMZPURbCZI9QYXCOKAmbbw2NS3dXcCnzKwWWAL8baYXMrOFZrbczJbX19dno1aRnPOjP7yNO3zhUm0lSHYFFQqZzq7xTuM3Az9z9/HAPOAXZva++tz9PnevcveqioqKLJQqklvqDrWwaFkNN54/nvHD+oddjhS4oEKhFkg/F3887+8e+hzwKIC7vwqUATozR/q8B17dSns8wV9dpq0Eyb6gQmEZMNXMJplZCckdyYs7tdkOzAYws2kkQ0H9Q9KntbTHeej17Vw1bRSnjBgQdjnSBwQSCu4eA24HngY2kDzKaJ2Z3W1m81PNvgLcZmargIeBz7h75y4mkT7ltyt2cKCpnVsvnhR2KdJHBHbnNXdfQnIHcvq0O9OG1wMXB1WPSK5zd+5/5R2mjRnMrMnDwy5H+gid0SySo/749j7e2tPIrRdP1H2XJTAKBZEcdf/L7zBiQAnzzx0bdinShygURHLQ1r1HeG5THZ+8aAJlxdGwy5E+RKEgkoMeemM7UTM+NeuUsEuRPkahIJJj2mIJfvNmLbOnjWTkYF3SQoKlUBDJMUvX72HfkTYWXDgh7FKkD1IoiOSYRcu2M25oPy6Zqsu4SPAUCiI5pGZ/Ey9t3svHqsYTjegwVAmeQkEkhzy6vAYzuKmq8viNRbJAoSCSI2LxBI8ur+HS0yoYO7Rf2OVIH6VQEMkRf3irnj2HWllwgXYwS3gUCiI54jd/qmXEgBJmTxsZdinShykURHJAQ3M7v99Qx5+fO5biqP4tJTxa+0RywJNrdtEWS3DDjM53qRUJlkJBJAc8tmIHk8sHcM74IWGXIn2cQkEkZLUHmnj9nf3cMGOcLpEtoVMoiITsdyuTtyu/Xl1HkgMUCiIhcnceW7GDCyYOo3J4/7DLEVEoiIRp3c5DVNc1aitBcoZCQSREv12xg+Ko8eGzx4RdigigUBAJTSLhPLFmF5dMrWBo/5KwyxEBFAoioVlRc4BdDS1ce662EiR3KBREQvL46l2UFEW4ctqosEsR6aBQEAlBIuEsWbOLS0+rYFBZcdjliHRQKIiEYPm2A+w51Mq156jrSHKLQkEkBE+s3klpUYTZ6jqSHKNQEAlYPOEsWbubK84YycDSorDLEXkPhYJIwN54Zz/1h1v5sLqOJAcpFEQC9sSanZQVR7jiDN1MR3KPQkEkQImE8/S6PVx++kj6l6jrSHJPYKFgZnPNbJOZVZvZHV20ucnM1pvZOjN7KKjaRIKyouYA9YdbmXvW6LBLEckokK8qZhYF7gWuAmqBZWa22N3Xp7WZCnwVuNjdD5iZtq2l4Dy1djclUXUdSe4KakvhQqDa3be4exuwCLiuU5vbgHvd/QCAu9cFVJtIINydp9bt5uIpI3TCmuSsoEJhHFCTNl6bmpbuNOA0M3vFzF4zs7mZXsjMFprZcjNbXl9fn6VyRXrf+l2HqNnfrK4jyWlBhUKmewx6p/EiYCpwGXAz8GMzG/q+J7nf5+5V7l5VUVHR64WKZMtTa3cTMXStI8lpQYVCLVCZNj4e2Jmhze/cvd3d3wE2kQwJkYLw1NrdXDRpBCMGloZdikiXggqFZcBUM5tkZiXAAmBxpza/BS4HMLNykt1JWwKqTySrqusa2VzXqK4jyXmBhIK7x4DbgaeBDcCj7r7OzO42s/mpZk8D+8xsPfA88A/uvi+I+kSy7el1uwGYc6a6jiS3BXb2jLsvAZZ0mnZn2rADX049RArKM+t2c27lUMYM6Rd2KSLHpDOaRbJsd0MLq2obuFpbCZIHFAoiWbZ0wx4A5kxXKEjuUyiIZNkz63YzuXwAp1YMDLsUkeNSKIhk0aGWdl7bso+rpo/CLNPpOiK5RaEgkkUvbKqnPe466kjyhkJBJIueWbeb8oGlnFc5LOxSRLpFoSCSJa2xOC9squfKaSOJRtR1JPmhx6FgZgNSl8IWkWN4bct+Gltj6jqSvHLcUDCziJl9wsyeMLM6YCOwK3UjnO+m7oMgIp08s243/UuifPDU8rBLEem27mwpPA+cSvIGOKPdvdLdRwIfAl4Dvm1mn8pijSJ5J5Fwlq7fw6WnVVBWrA1ryR/duczFle7e3nmiu+8HfgP8xsx0xxCRNGt2NFB3uJWrdMKa5JnjbikcDQQz+zfr4kDrTKEh0pctXb+HaMR0203JOz3Z0dwILDazAQBmNsfMXslOWSL5ben6PVwwcRhD+5eEXYpIj3T7Kqnu/nUz+wTwgpm1AkeAO7JWmUie2r6viU17DvNP104PuxSRHut2KJjZbOA2kmEwBvicu2/KVmEi+eqZ9al7J2h/guShnnQffQ34J3e/DLgReMTMrshKVSJ5bOn6PZwxehCVw/uHXYpIj3U7FNz9Cnd/OTW8BrgG+Fa2ChPJRweOtLFs634ddSR5qzsnr3V1xNEuYPax2oj0Nc9trCPhKBQkb3VnS+E5M/tbM5uQPtHMSoAPmNkDwC1ZqU4kzyxdv4fRg8s4e9yQsEsROSHd2dG8GYgDj5nZGOAgUAZEgWeAf3X3ldkrUSQ/tLTHeXFzPR+ZOU73TpC81Z1Q+KC7LzSzvwQmABVAs7sfzG5pIvnlj2/vpaktzpzpo8MuReSEdaf76GkzexUYBXwaGAu0ZLUqkTz0zLo9DCotYtbkEWGXInLCjrul4O5fMbPJwAvAJGA+cKaZtQFr3f3j2S1RJPfFE87vN+zhsjNGUlKk25RI/urWyWvuvsXMrnT3t45OM7OBwFlZq0wkj6zYfoC9jW06YU3yXk8uc/FWp/FGkpfOFunzlq7fQ3HUuOz0irBLETkp2s4VOUnuztPrdvOBU8sZVKaryEt+UyiInKTquka27mtS15EUBIWCyEl6Zv0eQGcxS2FQKIicpGfW7+HcyqGMGlwWdikiJ02hIHISdh5sZlXNQXUdScFQKIichGfWJe+dcM1ZOotZCkNgoWBmc81sk5lVm1mXd2wzsxvNzM2sKqjaRE7UU+t2c9qogUyuGBh2KSK9IpBQMLMocC/JezBMB242s/fdq9DMBgF/B7weRF0iJ2NfYytvvLOfuWdqK0EKR1BbChcC1e6+xd3bgEXAdRnafRO4B11bSfLA7zfsIeFwtbqOpIAEFQrjgJq08drUtA5mNgOodPfHj/VCZrbQzJab2fL6+vrer1Skm55au5sJw/szfczgsEsR6TVBhUKmi8t7x0yzCPCvwFeO90Lufp+7V7l7VUWFLikg4TjU0s7L1XuZe9Zo3TtBCkpQoVALVKaNjwd2po0PInlxvRfMbCswC1isnc2Sq57fWEd73Lla+xOkwAQVCsuAqWY2KXUbzwXA4qMz3b3B3cvdfaK7TyR5ob357r48oPpEeuSptbsZOaiUGZVDwy5FpFcFEgruHgNuB54GNgCPuvs6M7vbzOYHUYNIb2lqi/HCpnquPnM0kYi6jqSwdPvS2SfL3ZcASzpNu7OLtpcFUZPIiXh+Yz3N7XHmnT0m7FJEep3OaBbpocdX76RiUCkXThoedikivU6hINIDR1pjPLexjnlnjSaqriMpQAoFkR54dmMdrbEEHz5nbNiliGSFQkGkBx5ftZNRg0upOmVY2KWIZIVCQaSbDre088Jb9cw7e4yOOpKCpVAQ6aZnN9TRFktw7Tk66kgKl0JBpJseX72TsUPKmFGpriMpXAoFkW5oaGrnxbf2co26jqTAKRREumHJ2l20xRNcf9644zcWyWMKBZFueOxPOzi1YgBnjdNlsqWwKRREjqNmfxNvbN3PR2aO12WypeApFESO43crdwAw/1ydsCaFT6EgcgzuzmMrdnDhxOFUDu8fdjkiWadQEDmGNTsaeLv+CDfM1A5m6RsUCiLH8NiKHZREI8w7SyesSd+gUBDpQiye4H9W7WT2tJEM6V8cdjkigVAoiHThuY117G1s44YZ6jqSvkOhINKFR5bVUDGolMvPGBl2KSKBUSiIZLCroZnnN9XxsfPHUxzVv4n0HVrbRTL41fJaEg4fv6Ay7FJEAqVQEOkkkXAeWVbDxVNGcMqIAWGXIxIohYJIJy9V72XHwWYWXDAh7FJEAqdQEOnkkWXbGda/mDlnjgq7FJHAKRRE0tQfbmXp+j18ZOZ4SouiYZcjEjiFgkiah17fTnvc+cRF6jqSvkmhIJLSFkvwy9e3cdnpFZxaMTDsckRCoVAQSXlizU7qD7dy68WTwi5FJDQKBRGSl8i+/+WtTBk5kEumloddjkhoFAoiwJvbDrBmRwOf+eBE3V1N+jSFggjw01e2MqRfMR/RfROkjwssFMxsrpltMrNqM7sjw/wvm9l6M1ttZs+a2SlB1SZ9246DzTy1bjcLLqykf0lR2OWIhCqQUDCzKHAvcA0wHbjZzKZ3arYCqHL3c4BfA/cEUZvIj/7wNhGDWz4wMexSREIX1JbChUC1u29x9zZgEXBdegN3f97dm1KjrwHjA6pN+rA9h1pYtKyGG88fz9ih/cIuRyR0QYXCOKAmbbw2Na0rnwOezDTDzBaa2XIzW15fX9+LJUpf9KM/bCGecP7q0ilhlyKSE4IKhUyHc3jGhmafAqqA72aa7+73uXuVu1dVVFT0YonS1+xtbOWhN7Zx/XnjmDCif9jliOSEoPaq1QLpF6YfD+zs3MjMrgS+Blzq7q0B1SZ91H+9tIW2WIK/ufzUsEsRyRlBbSksA6aa2SQzKwEWAIvTG5jZDOBHwHx3rwuoLumjDhxp4xevbuPPzx3LZF3SQqRDIKHg7jHgduBpYAPwqLuvM7O7zWx+qtl3gYHAr8xspZkt7uLlRE7avc9X09we5/bLtS9BJF1gB2W7+xJgSadpd6YNXxlULdK3bdt3hAde3cpN51cyddSgsMsRySk6o1n6nHue2kRRJMKX55wWdikiOUehIH3Km9v288SaXXz+0smMGlwWdjkiOUehIH2Gu/OtJzYwclApCy+ZHHY5IjlJoSB9xuJVO1mx/SB/P+d0XeNIpAsKBekTDja18c3H13PO+CF89HxdQUWkK/q6JH3CPz+xgQNN7fz8sxcRjeh+CSJd0ZaCFLyXN+/lV2/W8vlLJjN97OCwyxHJaQoFKWjNbXH+8bE1TCofwN/Nnhp2OSI5T91HUtC+/eQGtu9vYtHCWZQVR8MuRyTnaUtBCtZTa3fxwKvb+OzFk5g1eUTY5YjkBYWCFKSa/U38w69Xc+74IdxxzRlhlyOSNxQKUnDaYgluf3gFAP/xiZmUFGk1F+ku7VOQguLufPPx9ayqOch/fnImlcN18xyRntBXKCkoP3n5HX7x2jYWXjKZa84eE3Y5InlHoSAFY8maXXzriQ3MO3s0d8zVfgSRE6FQkIKwfOt+vvTISs4/ZRjfu+k8IjprWeSEKBQk7y3bup/P/HQZ44b2478+XaXzEUROgkJB8tof397Lp3/yBiMHl/LwbbMYPqAk7JJE8ppCQfLWC5vquPWnyxg/rB+LFs5i9BDdNEfkZOmQVMk77s5PX9nKt55Yz+mjB/PLz13IiIGlYZclUhAUCpJXWmNxvv7YWn71Zi1zpo/iex8/j4GlWo1Feov+myRvvF3fyJcfWcmq2gb+7oopfOnK03SUkUgvUyhIzksknAde3cq3n9xIv5IoP/zUTOaepRPTRLJBoSA5bf3OQ9z1P+t44539XH56Bd/56DmMHKwdyiLZolCQnFR/uJXvLd3EomU1DOlXzLc/cjYfv6ASM3UXiWSTQkFyyu6GFn780hYeemM7bbEEt35wEl+cPZUh/YvDLk2kT1AoSOjcnTU7Gnjwte08tmIHcXfmnzuW26+YwqkVA8MuT6RPUShIaOoOt/Dkmt08sqyG9bsOUVYc4WNV4/nCpafqktciIVEoSGDcnbfrG/nDW3t5au0ulm87gDucOXYw37z+LOafO5Yh/dRNJBImhYJkTSLhbK5r5E/bD7B86wFeqd7L7kMtAJwxehBfnD2Va84aw+mjB4VcqYgcpVCQk+bu1De28k79Ed6uP8LG3YfYsOsQG3YdprE1BsCw/sV88NRyLp5Szoemlqt7SCRHBRYKZjYX+HcgCvzY3b/daX4p8HPgfGAf8HF33xpUfZJZPOEcaGpj/5E29ja2UneolT2HWtjV0MKOg83UHmimdn8Th1Mf/gADS4s4Y/QgbpgxjvMqhzLzlGFMHNFfh5OK5IFAQsHMosC9wFVALbDMzBa7+/q0Zp8DDrj7FDNbAHwH+HgQ9eULdyeecOJHf6YesYQTizvt8URqOEFrLEF7PEFbLEFb6mdrLEFLe5yW9gTN7XGa22I0tcVpaovT2BqjsSVGY2uMQy3tHGxqp6G5nUMt7bi/v5YBJVHGD+vPuGH9uGDiMCaVD2ByxUAmlw9g/LB+CgCRPBXUlsKFQLW7bwEws0XAdUB6KFwH3JUa/jXwH2Zm7pk+kk7Oo8tquO+lLR3jXf0K72Lk6KC7pw3D0TF33vNBmqldoqNNcjjhjnf6mXAnkUgOx1PTe1tRxOhXEmVQaREDy4oYWFrE8AElTCofwJB+xQztX8KIASUMH1DCiIEljBpcxqjBZboInUiBCuo/exxQkzZeC1zUVRt3j5lZAzAC2JveyMwWAgsBJkyYcELFDBtQwumjOu3c7OKLbfrk9G+/1jEtfdjebW9wdOxom6NPN4xIJDVkEDXraBOJGJHU60QjhpkRseRwxIxoJO1hRlHUKIoY0UiEoqhRHDWKIhFKiiKURCMURyOUFkcoLUpO61ccpaw4SllRlH4lUUqKdEsNEXlXUKGQ6SO38/fe7rTB3e8D7gOoqqo6oe/OV00fxVXTR53IU0VEClpQXxNrgcq08fHAzq7amFkRMATYH0h1IiICBBcKy4CpZjbJzEqABcDiTm0WA7ekhm8EnsvG/gQREelaIN1HqX0EtwNPkzwk9X53X2dmdwPL3X0x8BPgF2ZWTXILYUEQtYmIyLsCO4TE3ZcASzpNuzNtuAX4WFD1iIjI++nQExER6aBQEBGRDgoFERHpoFAQEZEOls9HfZpZPbDtBJ9eTqezpXOE6uoZ1dVzuVqb6uqZk6nrFHevyDQjr0PhZJjZcnevCruOzlRXz6iunsvV2lRXz2SrLnUfiYhIB4WCiIh06MuhcF/YBXRBdfWM6uq5XK1NdfVMVurqs/sURETk/fryloKIiHSiUBARkQ4FHQpm9jEzW2dmCTOr6jTvq2ZWbWabzOzqLp4/ycxeN7PNZvZI6rLfvV3jI2a2MvXYamYru2i31czWpNot7+06Mvy+u8xsR1pt87poNze1DKvN7I4A6vqumW00s9Vm9piZDe2iXSDL63h/v5mVpt7j6tS6NDFbtaT9zkoze97MNqTW/y9maHOZmTWkvb93ZnqtLNR2zPfFkr6fWl6rzWxmADWdnrYcVprZITP7Uqc2gS0vM7vfzOrMbG3atOFmtjT1WbTUzIZ18dxbUm02m9ktmdocl7sX7AOYBpwOvABUpU2fDqwCSoFJwNtANMPzHwUWpIZ/CPxVluv9v8CdXczbCpQHuOzuAv7+OG2iqWU3GShJLdPpWa5rDlCUGv4O8J2wlld3/n7gr4EfpoYXAI8E8N6NAWamhgcBb2Wo6zLg8aDWp+6+L8A84EmSd2KcBbwecH1RYDfJk7tCWV7AJcBMYG3atHuAO1LDd2Ra74HhwJbUz2Gp4WE9/f0FvaXg7hvcfVOGWdcBi9y91d3fAaqBC9MbWPKGzFcAv05NegC4Plu1pn7fTcDD2fodWXAhUO3uW9y9DVhEctlmjbs/4+6x1OhrJO/iF5bu/P3XkVx3ILkuzbb0m31ngbvvcvc/pYYPAxtI3gM9H1wH/NyTXgOGmtmYAH//bOBtdz/RKyWcNHd/kfffdTJ9Perqs+hqYKm773f3A8BSYG5Pf39Bh8IxjANq0sZref8/zQjgYNoHUKY2velDwB5339zFfAeeMbM3zWxhFutId3tqE/7+LjZXu7Mcs+mzJL9VZhLE8urO39/RJrUuNZBctwKR6q6aAbyeYfYHzGyVmT1pZmcGVNLx3pew16kFdP3FLIzlddQod98FydAHRmZo0yvLLrCb7GSLmf0eGJ1h1tfc/XddPS3DtM7H5nanTbd0s8abOfZWwsXuvtPMRgJLzWxj6hvFCTtWXcB/At8k+Td/k2TX1mc7v0SG5570Mc7dWV5m9jUgBjzYxcv0+vLKVGqGaVlbj3rKzAYCvwG+5O6HOs3+E8kuksbU/qLfAlMDKOt470uYy6sEmA98NcPssJZXT/TKssv7UHD3K0/gabVAZdr4eGBnpzZ7SW66FqW+4WVq0ys1mlkR8BHg/GO8xs7Uzzoze4xk18VJfch1d9mZ2X8Bj2eY1Z3l2Ot1pXagXQvM9lRnaobX6PXllUF3/v6jbWpT74zC0/IAAAKrSURBVPMQ3t810OvMrJhkIDzo7v/deX56SLj7EjP7gZmVu3tWL/zWjfclK+tUN10D/Mnd93SeEdbySrPHzMa4+65Ud1pdhja1JPd9HDWe5P7UHumr3UeLgQWpI0MmkUz8N9IbpD5sngduTE26Behqy+NkXQlsdPfaTDPNbICZDTo6THJn69pMbXtLp37cG7r4fcuAqZY8SquE5Kb34izXNRf4P8B8d2/qok1Qy6s7f/9ikusOJNel57oKst6S2mfxE2CDu3+vizajj+7bMLMLSX4W7MtyXd15XxYDn04dhTQLaDjabRKALrfWw1henaSvR119Fj0NzDGzYanu3jmpaT0TxN70sB4kP8xqgVZgD/B02ryvkTxyZBNwTdr0JcDY1PBkkmFRDfwKKM1SnT8DvtBp2lhgSVodq1KPdSS7UbK97H4BrAFWp1bIMZ3rSo3PI3l0y9sB1VVNst90Zerxw851Bbm8Mv39wN0kQwugLLXuVKfWpckBLKM/I9ltsDptOc0DvnB0PQNuTy2bVSR32H8wgLoyvi+d6jLg3tTyXEPaUYNZrq0/yQ/5IWnTQlleJINpF9Ce+vz6HMn9UM8Cm1M/h6faVgE/TnvuZ1PrWjVw64n8fl3mQkREOvTV7iMREclAoSAiIh0UCiIi0kGhICIiHRQKIiLSQaEgIiIdFAoiItJBoSDSiyx5H4OrUsPfMrPvh12TSE/k/bWPRHLMN4C7Uxd8m0HyAmsieUNnNIv0MjP7AzAQuMyT9zMQyRvqPhLpRWZ2Nsk7n7UqECQfKRREeknqyrIPkrxL1hHr4t7fIrlMoSDSC8ysP/DfwFfcfQPJGxPdFWpRIidA+xRERKSDthRERKSDQkFERDooFEREpINCQUREOigURESkg0JBREQ6KBRERKTD/wd5XTRva+DMCgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Sigmoid function ###\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "x = np.linspace(-10, 10, 100)\n",
    "\n",
    "plt.plot(x, sigmoid(x))\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "x = np.linspace(-10, 10, 100)\n",
    "plt.plot(x, ReLU(x))\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.show();\n",
    "#y = max(0, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have here a fully connected layer, the connections between layers actually represent **2 weight matrices**:\n",
    "\n",
    "<img src=\"nn_w.png\" alt=\"ffcf\" style=\"width: 800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the matrix notation:\n",
    "\n",
    "<img src=\"m_notation.png\" alt=\"mn\" style=\"width: 800px;\">    \n",
    "\n",
    "And a single element computation:\n",
    "$$a'_{0}= f(w_{0,0}a_{0}+w_{0,1}a_{1}+w_{0,2}a_{2}+w_{0,3}a_{3} +...+ b_{0})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{pmatrix} \n",
    "a'_{0}\\\\ a'_{1}\\\\ \n",
    "\\vdots \\\\ \n",
    "a'_{h}\\\\\n",
    "\\end{pmatrix} =\n",
    "f \\Bigg(\n",
    "\\begin{pmatrix} \n",
    "w_{0,0} & w_{0,1} & \\ldots & w_{0,k} \\\\ \n",
    "w_{1,0} & w_{1,1} &\\ldots & w_{1,k}\\\\ \n",
    "w_{2,0} & w_{2,1} &\\ldots & w_{2,k}\\\\ \n",
    "\\vdots & \\vdots & \\ldots & \\vdots\\\\\n",
    "w_{h,0} & w_{h,1} & \\ldots & w_{h,k}\\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "a_{0}\\\\ a_{1}\\\\ \\vdots\\\\ a_{k}\\\\\n",
    "\\end{pmatrix}+\n",
    "\\begin{pmatrix}b_{0}\\\\  b_{1}\\\\\n",
    "\\vdots\\\\\n",
    "b_{h}\\\\\n",
    "\\end{pmatrix}\n",
    "\\Bigg)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting output will not quite give us the desired result. \n",
    "The above computations will provide us with **scores**, while we are in need of probabilities, \n",
    "and not just probabilities, but the distribution of probabilities over the classes of\n",
    " our target variable. That's where **softmax** comes in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 softmax function ##\n",
    "\n",
    "$$S(x_{i})=\\frac{e^{x_{i}}}{\\sum_{j} e^{x_{i}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.93320441e-45 1.21225154e-05 1.61953494e-01 2.41606222e-01\n",
      " 2.67016170e-01 3.29411991e-01]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.    , 0.    , 0.162 , 0.2416, 0.267 , 0.3294])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As a result of our matrix multiplication we got these:\n",
    "\n",
    "scores = [-100, -10, -0.5, -0.1, 0, 0.21]\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "print(softmax(scores))\n",
    "np.round(softmax(scores),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#our observation can only be labelled as one class, so the probabilities should sum up to 1, check it\n",
    "sum(softmax(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"softmax.png\" alt=\"ffff\" style=\"width: 600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: [RitchiNg.com](https://www.ritchieng.com/machine-learning/deep-learning/neural-nets/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.6 application##\n",
    "Let's try to set up the NN architecture on our own! We have already loaded our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7197"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the data\n",
    "len(app) # that would be the number of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "size_bytes                      -0.525218\n",
       "price                           -0.126225\n",
       "rating_count_tot                -0.061266\n",
       "sup_devices.num                  2.578809\n",
       "ipadSc_urls.num                  0.651051\n",
       "lang.num                        -0.560030\n",
       "vpp_lic                          0.083642\n",
       "currency_USD                     1.000000\n",
       "cont_rating_12+                  0.000000\n",
       "cont_rating_17+                  0.000000\n",
       "cont_rating_4+                   1.000000\n",
       "cont_rating_9+                   0.000000\n",
       "prime_genre_Book                 0.000000\n",
       "prime_genre_Business             0.000000\n",
       "prime_genre_Catalogs             0.000000\n",
       "prime_genre_Education            0.000000\n",
       "prime_genre_Entertainment        0.000000\n",
       "prime_genre_Finance              0.000000\n",
       "prime_genre_Food & Drink         0.000000\n",
       "prime_genre_Games                1.000000\n",
       "prime_genre_Health & Fitness     0.000000\n",
       "prime_genre_Lifestyle            0.000000\n",
       "prime_genre_Medical              0.000000\n",
       "prime_genre_Music                0.000000\n",
       "prime_genre_Navigation           0.000000\n",
       "prime_genre_News                 0.000000\n",
       "prime_genre_Photo & Video        0.000000\n",
       "prime_genre_Productivity         0.000000\n",
       "prime_genre_Reference            0.000000\n",
       "prime_genre_Shopping             0.000000\n",
       "prime_genre_Social Networking    0.000000\n",
       "prime_genre_Sports               0.000000\n",
       "prime_genre_Travel               0.000000\n",
       "prime_genre_Utilities            0.000000\n",
       "prime_genre_Weather              0.000000\n",
       "user_rating                      4.000000\n",
       "Name: Shanghai Mahjong, dtype: float64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And here is one observation\n",
    "app.iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(app[\"user_rating\"])\n",
    "encoded_Y = encoder.transform(app[\"user_rating\"])\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "dummy_y# 6 levels -> 6 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 5., 3., 2., 0., 1.])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app[\"user_rating\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array(app)\n",
    "X = data[:,:-1]\n",
    "y = dummy_y.astype(int)\n",
    "y[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7197, 35) (7197, 6)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's set up the structure of our ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Determine the structure of the NN\n",
    "inputLayer = 35 # by the number of features - a\n",
    "hiddenLayer = 50 #hidden layer, as we decided - a'\n",
    "outputLayer = 6 #output, by the number of levels - a''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To kickstart the NN we need to set up the weight matrices to some numbers. Setting them randomly won't yield good resuls, we will try the uniformly distributed ones for now but in the future we will talk on better ways\n",
    "\n",
    "<img src=\"nn_ws.png\" alt=\"ffcf\" style=\"width: 800px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Randomly defining the weights between nodes won't yield a good result \n",
    "#weightsInputToHidden = np.random.rand(hiddenLayer,inputLayer)\n",
    "#weightsHiddenToOutput = np.random.rand(outputLayer, hiddenLayer)\n",
    "np.random.seed(9)\n",
    "\n",
    "limit = np.sqrt(6 / (inputLayer + outputLayer)) #will be discussed during the lecture\n",
    "\n",
    "weightsInputToHidden  = np.random.uniform(-limit, limit, (hiddenLayer, inputLayer)) # W\n",
    "weightsHiddenToOutput = np.random.uniform(-limit, limit, (outputLayer, hiddenLayer)) # M\n",
    "\n",
    "biasInputToHidden  = np.ones( (hiddenLayer,1) ) #for sigmoid to pick 0, for ReLu you pick one - b\n",
    "biasHiddenToOutput = np.ones( (outputLayer,1) ) #  v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will be inputing just one observation for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "size_bytes                      -0.525218\n",
       "price                           -0.126225\n",
       "rating_count_tot                -0.061266\n",
       "sup_devices.num                  2.578809\n",
       "ipadSc_urls.num                  0.651051\n",
       "lang.num                        -0.560030\n",
       "vpp_lic                          0.083642\n",
       "currency_USD                     1.000000\n",
       "cont_rating_12+                  0.000000\n",
       "cont_rating_17+                  0.000000\n",
       "cont_rating_4+                   1.000000\n",
       "cont_rating_9+                   0.000000\n",
       "prime_genre_Book                 0.000000\n",
       "prime_genre_Business             0.000000\n",
       "prime_genre_Catalogs             0.000000\n",
       "prime_genre_Education            0.000000\n",
       "prime_genre_Entertainment        0.000000\n",
       "prime_genre_Finance              0.000000\n",
       "prime_genre_Food & Drink         0.000000\n",
       "prime_genre_Games                1.000000\n",
       "prime_genre_Health & Fitness     0.000000\n",
       "prime_genre_Lifestyle            0.000000\n",
       "prime_genre_Medical              0.000000\n",
       "prime_genre_Music                0.000000\n",
       "prime_genre_Navigation           0.000000\n",
       "prime_genre_News                 0.000000\n",
       "prime_genre_Photo & Video        0.000000\n",
       "prime_genre_Productivity         0.000000\n",
       "prime_genre_Reference            0.000000\n",
       "prime_genre_Shopping             0.000000\n",
       "prime_genre_Social Networking    0.000000\n",
       "prime_genre_Sports               0.000000\n",
       "prime_genre_Travel               0.000000\n",
       "prime_genre_Utilities            0.000000\n",
       "prime_genre_Weather              0.000000\n",
       "user_rating                      4.000000\n",
       "Name: Shanghai Mahjong, dtype: float64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X[5]\n",
    "app.iloc[5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(35, 1)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to ensure that the input comes as an array of correct dimension\n",
    "print(X[5].shape)\n",
    "# This will not work since, for matrix multiplication, we need a kx1 vector\n",
    "inputs = np.array(X[5]).reshape( (35,1) )\n",
    "inputs.shape # now we have a nice and sweet input vector!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.52521775],\n",
       "       [-0.12622463],\n",
       "       [-0.06126572],\n",
       "       [ 2.57880861],\n",
       "       [ 0.65105067],\n",
       "       [-0.56003026],\n",
       "       [ 0.08364175],\n",
       "       [ 1.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 1.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 1.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6, 1)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we need to make sure our output vectors will be what we want it to be. \n",
    "# That is basically the same operation as above\n",
    "print(y[5].shape)\n",
    "target = y[5].reshape( (6,1) )\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50, 35), (35, 1), (6, 50), (6, 1))"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show matrices of weights\n",
    "weightsInputToHidden.shape, inputs.shape, weightsHiddenToOutput.shape, target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we are all set, let's multiply!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hL_inputs = np.dot(weightsInputToHidden, inputs) + biasInputToHidden\n",
    "hL_outputs = ReLU(hL_inputs)\n",
    "hL_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.36172534],\n",
       "       [-0.69414103],\n",
       "       [ 0.46249297],\n",
       "       [ 0.21754509],\n",
       "       [ 2.93789853],\n",
       "       [ 1.10027884]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oL_inputs = np.dot(weightsHiddenToOutput, hL_outputs) + biasHiddenToOutput\n",
    "oL_inputs\n",
    "#doesn't look like probabilities, that's because these are our scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13405368],\n",
       "       [0.01715645],\n",
       "       [0.05454401],\n",
       "       [0.04269407],\n",
       "       [0.64833888],\n",
       "       [0.1032129 ]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oL_outputs = softmax(oL_inputs)\n",
    "oL_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and just for verification\n",
    "np.sum(oL_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## So what was the label again?\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Our model is somewhat on the way but still a little uncertain, we need to give it a nudge in the right direction. So how do you usually tell the model it's wrong? You show it the evaluation metric and try to make it better.\n",
    "\n",
    "To evaluate the the probabilities estimated by the model, we need to compare them with the true values, \n",
    "that are one-hot encoded in our case. So we need to compare our outputs to \n",
    "\n",
    "[ 0, 0, 0, 0, 1, 0]\n",
    "\n",
    "You can imagine that we cannot use MSE here, as we are dealing with a certain probability distribution.\n",
    "Our *loss function* has to grasp the difference between these probabilities. This is where the **cross-entropy loss**\n",
    "function will come in handy. It is used when the output of a model represents the probability of an outcome, i.e. when the output is a probability distribution. \n",
    "It is used as a loss function in neural networks that have softmax activations in the output layer.\n",
    "\n",
    "*Remark*: in case you are interested, there a nice article (with codes) on the differences between squared-error and cross-entropy\n",
    "in the context of training neural networks appeared in the [Visual Study magazine](https://visualstudiomagazine.com/articles/2017/07/01/cross-entropy.aspx)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: make it work # \n",
    " 1. loss function\n",
    " 2. gradient\n",
    " 4. batches\n",
    " 5. learning rate\n",
    " 6. stochastic gradient descent\n",
    " 7. backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Loss function\n",
    "### Logistic regression case\n",
    "Let us revisit classic logistic regression. We build a linear model to distinguish between two classes. So our target variable can only take one of two states $ y \\in \\{0,1\\} $.\n",
    "\n",
    "With $ w$ denoting the parameters (aka weights) of our logit model and letting observations be indexed by $i = 1, 2, ..., n$, our loss function $J(w)$ has the form:\n",
    "\n",
    "$$J(w) = \\sum_{i=1}^{n} y^{(i)} \\log P(y=1) + (1 - y^{(i)}) \\log P(y=0)$$\n",
    "Where P(y) represent the probability of a certain binary outcome\n",
    "\n",
    "We will however consider another loss function - cross entropy. It is used when the output represents the probability of an outcome, i.e. when the output is a probability distribution. It is used as a loss function in neural networks that have softmax activations in the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-entropy \n",
    "Entropy (H(y)) is a term from Information Theory. It had a great impact on the field of communication and signifies the optimal number of bits to encode a certain information content ($y_c$ is the probability of the c-th event, symbol or in our case class):\n",
    "\n",
    "$$H(y) = \\sum_c y_c \\log \\frac{1}{y_c} = -\\sum_c y_c \\log y_c$$\n",
    "\n",
    "Now the cross-entropy (H(y,\\hat{y})) is the number of bits we'll need if we encode symbols from $y$ using the wrong tool $\\hat{y}$. Cross entropy is always bigger or equal to entropy. Mind that $c$ stands for the number of classes. \n",
    "\n",
    "$$H(y, \\hat{y}) = \\sum_c y_c \\log \\frac{1}{\\hat{y}_c} = -\\sum_c y_c \\log \\hat{y}_c$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly enough, the The KL divergence that you have encountered before in BADS (uplift random forest) is simply the difference between cross entropy and entropy:\n",
    "$$\\mbox{KL}(y~||~\\hat{y})\n",
    "= \\sum_c y_c \\log \\frac{1}{\\hat{y}_c} - \\sum_c y_c \\log \\frac{1}{y_c}\n",
    "= \\sum_c y_c \\log \\frac{y_c}{\\hat{y}_c}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would be calculating the cross-entropy for every pair of true/estimated probabilities and averaging it over the sample or batch (more about it later) - this will be our loss function *L* that we will ultimately want to minimise (class i, smaple j):\n",
    "\n",
    "$$L=-\\frac{1}{N}\\sum_i \\sum_c y_{i,c} \\log(\\hat{y}_{i,c})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CE example\n",
    "A concrete example is the best way to explain the purely mathematical form of CE. Suppose you have a weirdly shaped\n",
    "four-sided dice (yes, I know the singular is really \"die\"). Using some sort of intuition or physics, you predict\n",
    "that the probabilities of the four sides are (0.20, 0.40, 0.30, 0.10). \n",
    "You then roll the dice many thousands of times and determine that the true probabilities are  (0.15, 0.35, 0.25, 0.25). \n",
    "Here is how we calculate the CE error of our prediction:\n",
    "\n",
    "CE prediction error:\n",
    "-1.0 * [ ln(0.20) * 0.15 + ln(0.40) * 0.35 + ln(0.30) * 0.25 + ln(0.10) * 0.25 ] = \n",
    "-1.0 * [ (-1.61)(0.15) + (-0.92)(0.35) + (-1.20)(0.25) + (-2.30)(0.25) ] =\n",
    "1.44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy error of weird dice prediction is: 1.4388\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "ce=-1.0 * (np.log(0.20) * 0.15 + np.log(0.40) * 0.35 + np.log(0.30) * 0.25 + np.log(0.10) * 0.25)\n",
    "print('Cross entropy error of weird dice prediction is: {:.4f}'.format(ce))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us apply this idea to our network outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True label:  (6, 1)\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "Prediction:  (6, 1)\n",
      "[[0.134]\n",
      " [0.017]\n",
      " [0.055]\n",
      " [0.043]\n",
      " [0.648]\n",
      " [0.103]]\n"
     ]
    }
   ],
   "source": [
    "yhat=np.round(oL_outputs, 3) #that's our WX+b run through softmax\n",
    "print('True label: ',  target.shape)\n",
    "print(target)\n",
    "print('Prediction: ', yhat.shape)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4338645826298623\n"
     ]
    }
   ],
   "source": [
    "# Calculation of cross-entropy loss\n",
    "L =- sum(target*np.log(yhat))\n",
    "print(L.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4338645826298623\n"
     ]
    }
   ],
   "source": [
    "# or, if you prefer\n",
    "L=-1*np.dot(target.T, np.log(yhat))\n",
    "print(L.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the number we want the machine to minimize! But how?\n",
    "We ca not change our input, we cannot change our labels, what we can adjust are **weights and biases** - these are the parameters of our neural network. So we need to change our **W, M, b and v** and check if the loss decreases.\n",
    "\n",
    "NB: number of neurons in the hidden layer AND number of hidden layers are meta-parameters and certainly can be experimented with, we will discuss it further down the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most efficient way to move around the function slope is to find the derivative.\n",
    "\n",
    "In our naive example, we are dealing with 1 example only while we have 6 classes. In practice, these numbers, the number of examples and output classes would be much larger, making the calculation of the derivative of the loss function computationally demanding. \n",
    "\n",
    "And that was the point when everybody almost gave up on neural networks. Spoiler alert: the solutions were **stochastic gradient descent and backpropagation**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Gradient \n",
    "The gradient ( $\\nabla$) is a vector operation which operates on a scalar function to produce a vector whose magnitude is the maximum rate of change of the function at the point of the gradient and which is pointed in the direction of that maximum rate of change. \n",
    "\n",
    "Well, put in easier terms, gradient is a **vector of partial derivatives**. Why would we need it? Because we need the derivative of this:\n",
    "$$L=-\\frac{1}{N}\\sum\\sum y\\log(softmax(M(relu(Wx+b))+v)),$$\n",
    "\n",
    "where the functions $s$ and $r$ stand for softmax, and relu, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We want to adjust every estimation of y (yhat) so as to minimize the loss function. For that we would deduct a gradient of the loss function from it and continue doing these iterations until we reach the local minimum. \n",
    "$$L_{t+1} =L_t - \\eta \\cdot \\nabla L(W, M, b, v)$$\n",
    "\n",
    "You can think of of gradient as a list of directionsfor improvement (of course, imagining moving in 10000 directions is hard):\n",
    "$$ \\nabla L(W,M,b,v)=\\begin{bmatrix}  \\frac{dL}{dW}\\\\ \\frac{dL}{dM}\\\\ \\frac{dL}{db}\\\\ \\frac{dL}{dv}\\end{bmatrix}$$\n",
    "\n",
    "Normally it would be a result of averaging over all observations that went through with forward pass, but again- we have just one this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://giphy.com/embed/8tvzvXhB3wcmI\" width=\"1000\" height=\"400\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe>\n",
       "<p><a href=\"https://giphy.com/gifs/deep-learning-8tvzvXhB3wcmI\">via GIPHY</a></p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe src=\"https://giphy.com/embed/8tvzvXhB3wcmI\" width=\"1000\" height=\"400\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe>\n",
    "<p><a href=\"https://giphy.com/gifs/deep-learning-8tvzvXhB3wcmI\">via GIPHY</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient descent example\n",
    "For illustration purpose, we will try to find the local minimum of the function $ f(x) = \\left(x-4\\right)^2 $. In this case, the gradient is a simple derivative: $2 \\left(x-4\\right)$.\n",
    "\n",
    "The example draws on: https://towardsdatascience.com/implement-gradient-descent-in-python-9b93ed7108d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 05 with X equal to 5.9661\n",
      "Iteration 10 with X equal to 4.6442\n",
      "The local minimum occurs at: 4.6442450944\n"
     ]
    }
   ],
   "source": [
    "cur_x = 10 # Randomly guessed starting point \n",
    "rate = 0.1 # Learning rate\n",
    "\n",
    "# Let's impose two stopping conditions, a max. number\n",
    "# of iterations and a minimum step size\n",
    "precision = 0.000001 # threshold for the step size\n",
    "previous_step_size = 1 # change of x in the prev. step\n",
    "max_iters = 10 # maximum number of iterations\n",
    "iters = 0 #iteration counter\n",
    "\n",
    "df = lambda x: 2*(x-4) #Gradient of our function '\n",
    "\n",
    "while previous_step_size > precision and iters < max_iters:\n",
    "    prev_x = cur_x #store current x value in prev_x\n",
    "    cur_x = cur_x - rate * df(prev_x) # gradient descent step\n",
    "    previous_step_size = abs(cur_x - prev_x) # change in x\n",
    "    iters = iters+1 #iteration count\n",
    "    if (iters%5 == 0.0):\n",
    "        print(\"Iteration {:02d} with X equal to {:.4f}\".format(iters,cur_x)) #Print iterations\n",
    "    \n",
    "print(\"The local minimum occurs at: {:.10f}\".format(cur_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.3 updating weights\n",
    "The following codes sketch gradient descent for neural network learning. For simplicity, we consider the squared-error loss function. Its derivative is very easy and more handy compared to cross-entropy. However, the same concepts apply to the cross-entropy loss function or yet other loss functions.\n",
    "\n",
    "We also need the derivative of our activation function, which was ReLu, and the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def loss_derivative(output, y): #check the derivation if interested https://sefiks.com/2017/12/17/a-gentle-introduction-to-cross-entropy-loss-function/\n",
    "    return output - y\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return (x>0)*1.0\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.13405368],\n",
       "       [-0.01715645],\n",
       "       [-0.05454401],\n",
       "       [-0.04269407],\n",
       "       [ 0.35166112],\n",
       "       [-0.1032129 ]])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DELETE\n",
    "oL_errors = target - oL_outputs\n",
    "oL_errors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.51047741],\n",
       "       [ 0.02737673],\n",
       "       [ 0.04699093],\n",
       "       [ 0.05200084],\n",
       "       [ 0.27632279],\n",
       "       [-0.9131687 ]])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_derivative(oL_outputs, target) * ReLU_derivative(oL_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going on, let's remind us of our network architecture and the shape of relevant variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 50)\n",
      "(50, 35)\n",
      "(6, 1)\n",
      "(50, 1)\n"
     ]
    }
   ],
   "source": [
    "print(weightsHiddenToOutput.shape)\n",
    "print(weightsInputToHidden.shape)\n",
    "print(oL_errors.shape)\n",
    "print(hL_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# gradient for the weights between hidden and output layer\n",
    "gradient_HiddenToOutput = loss_derivative(oL_outputs, target) * ReLU_derivative(oL_outputs)\n",
    "oL_errors = gradient_HiddenToOutput\n",
    "# gradient for the weights between input and hidden layers\n",
    "gradient_InputToHidden = np.dot(weightsHiddenToOutput.T, oL_errors) * ReLU_derivative(hL_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# update the matrix for weights between hidden and output layers\n",
    "# define the learning rate\n",
    "learningRate = 0.0001\n",
    "\n",
    "biasHiddenToOutput -= learningRate * gradient_HiddenToOutput\n",
    "biasInputToHidden -= learningRate * gradient_InputToHidden\n",
    "\n",
    "weightsHiddenToOutput -= learningRate * np.dot(gradient_HiddenToOutput, np.transpose(hL_outputs))\n",
    "weightsInputToHidden -= learningRate * np.dot(gradient_InputToHidden, np.transpose(inputs)   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 stochastic gradient descent and backpropagation##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from derivatives, avereging over the whole training dataset might be costly. That is why **stochastic** gradient descent was introduced. It basically means that instead of using the full training set, the algorithm will only use a certain  random **batch** (size of this batch is a metaparameter like the number of neurons). This introduces a certain \"slopinness\" to the process but allows to run the **backpropagation** much faster. After many iterations, we expect to converge to the sample parameters anyway. Below, we exemplify backpropagation for our simple neural network. To that end, we require another important concept, an **epoch**. An epoch describes on run of the entire data set through the NN. \n",
    "\n",
    "Essentially, Backpropagation is an algorithm for computing the gradient in a multidimensional space. You can find a very description of the algorithm in the [ML Glossary](https://ml-cheatsheet.readthedocs.io/en/latest/backpropagation.html). Here is a picture illustrating the idea.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"nn_color.png\" alt=\"Backpropagation\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.5 Updating for each observation in the data (the original stochastic gradient descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "learningRate = 0.001\n",
    "epochs = 10 # how many times do we have to run the training set through the network\n",
    "\n",
    "input_dim = X.shape[1] # number of variables\n",
    "n_classes = y.shape[1]  # number of classes (i.e., 6 user ratings one-hot encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 with loss 0.4054\n",
      "Epoch 1 with loss 0.4052\n",
      "Epoch 2 with loss 0.4049\n",
      "Epoch 3 with loss 0.4046\n",
      "Epoch 4 with loss 0.4044\n",
      "Epoch 5 with loss 0.4041\n",
      "Epoch 6 with loss 0.4039\n",
      "Epoch 7 with loss 0.4037\n",
      "Epoch 8 with loss 0.4034\n",
      "Epoch 9 with loss 0.4032\n"
     ]
    }
   ],
   "source": [
    "iteration=0\n",
    "\n",
    "while iteration < epochs:\n",
    "    \n",
    "    # Put all the steps done before in a loop:\n",
    "    # Process one observation per iteration \n",
    "    squared_residuals = np.zeros(X.shape[0]) # to keep track of the loss\n",
    "    for i in range(X.shape[0]):\n",
    "\n",
    "        inputs  = np.array(X[i]).reshape( (input_dim,1) ) # select current case i\n",
    "        target = y[i].reshape( (n_classes,1) ) # and its target value\n",
    "                \n",
    "        # Compute the forward pass through the network all the way up to the final output  \n",
    "        hL_inputs = np.dot(weightsInputToHidden, inputs) + biasInputToHidden\n",
    "        hL_outputs = ReLU(hL_inputs)\n",
    "        oL_inputs = np.dot(weightsHiddenToOutput, hL_outputs) + biasHiddenToOutput \n",
    "        oL_outputs = softmax(oL_inputs) # final output: probabilistic prediction for case i \n",
    "\n",
    "        # Compute the gradients:\n",
    "        # gradient for the weights between hidden and output layers\n",
    "        gradient_HiddenToOutput = loss_derivative(oL_outputs, target) * ReLU_derivative(oL_outputs)\n",
    "        oL_errors = gradient_HiddenToOutput\n",
    "        \n",
    "        # gradient for the weights between input and hidden layers\n",
    "        gradient_InputToHidden = np.dot(weightsHiddenToOutput.T, oL_errors) * ReLU_derivative(hL_outputs)\n",
    "\n",
    "        # Perform gradient descent update\n",
    "        biasHiddenToOutput -= learningRate * gradient_HiddenToOutput\n",
    "        biasInputToHidden  -= learningRate * gradient_InputToHidden\n",
    "\n",
    "        weightsHiddenToOutput -= learningRate * np.dot(gradient_HiddenToOutput, np.transpose(hL_outputs))\n",
    "        weightsInputToHidden  -= learningRate * np.dot(gradient_InputToHidden, np.transpose(inputs))\n",
    "        \n",
    "        # Store residual \n",
    "        squared_residuals[i] = np.sum((target-oL_outputs)**2)\n",
    "        \n",
    "    # Development of the loss as average over obs-level losses\n",
    "    print('Epoch {} with loss {:.4f}'.format(iteration, np.mean(squared_residuals)))\n",
    "    iteration += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's check the results! ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Extract some 'test' data\n",
    "X_test = data[78:178,:-1]\n",
    "y_test = dummy_y[78:178].astype(int)\n",
    "y_test = np.argmax(dummy_y[78:178], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# scorecard of the network\n",
    "confusion_matrix = np.zeros( (6,6), dtype=\"int\" )\n",
    "\n",
    "# go through all the observations in the test data set\n",
    "for i in range(len(y_test)):\n",
    "    # compute the network output (using the trained weights)\n",
    "    inputs = np.array(X_test[i]).reshape( (35,1) )\n",
    "\n",
    "    hL_inputs = np.dot(weightsInputToHidden, inputs) + biasInputToHidden\n",
    "    hL_outputs = ReLU(hL_inputs)\n",
    "    oL_inputs = np.dot(weightsHiddenToOutput, hL_outputs) + biasHiddenToOutput\n",
    "    oL_outputs = softmax(oL_inputs)\n",
    "\n",
    "    # determine most likely class\n",
    "    label = np.argmax(oL_outputs)\n",
    "\n",
    "    confusion_matrix[label, y_test[i]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# scorecard of the network\n",
    "confusion_matrix = np.zeros( (6,6), dtype=\"int\" )\n",
    "true_labels = np.argmax(y, axis=1)\n",
    "pred_labels = np.zeros_like(true_labels)\n",
    "# go through all the observations in the test data set\n",
    "for i in range(len(y)):\n",
    "    # compute the network output (using the trained weights)\n",
    "    inputs = np.array(X[i]).reshape( (35,1) )\n",
    "\n",
    "    hL_inputs = np.dot(weightsInputToHidden, inputs) + biasInputToHidden\n",
    "    hL_outputs = ReLU(hL_inputs)\n",
    "    oL_inputs = np.dot(weightsHiddenToOutput, hL_outputs) + biasHiddenToOutput\n",
    "    oL_outputs = softmax(oL_inputs)\n",
    "\n",
    "    # determine most likely class\n",
    "    pred_labels[i] = np.argmax(oL_outputs)\n",
    "\n",
    "    confusion_matrix[pred_labels[i], true_labels[i]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 566,   17,  102,  105,  530,   97],\n",
       "       [   0,    0,    0,    0,    0,    0],\n",
       "       [  10,    1,   18,    7,   21,    7],\n",
       "       [   2,    0,    0,    3,    0,    0],\n",
       "       [ 347,   26,  235,  268, 4426,  382],\n",
       "       [   4,    0,    3,    0,   14,    6]])"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy of trained network: 0.70\n"
     ]
    }
   ],
   "source": [
    "# Calculate percentage correctly classified\n",
    "pcc = np.trace(confusion_matrix) / len(y)\n",
    "print('Classification accuracy of trained network: {:.2f}'.format(pcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, not too bad for our first neural network and a lot better than random guessing ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>144</td>\n",
       "      <td>12</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>803</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>169</td>\n",
       "      <td>7</td>\n",
       "      <td>48</td>\n",
       "      <td>75</td>\n",
       "      <td>840</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>163</td>\n",
       "      <td>5</td>\n",
       "      <td>74</td>\n",
       "      <td>54</td>\n",
       "      <td>859</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>134</td>\n",
       "      <td>7</td>\n",
       "      <td>64</td>\n",
       "      <td>62</td>\n",
       "      <td>846</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>154</td>\n",
       "      <td>4</td>\n",
       "      <td>62</td>\n",
       "      <td>75</td>\n",
       "      <td>818</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>165</td>\n",
       "      <td>9</td>\n",
       "      <td>49</td>\n",
       "      <td>56</td>\n",
       "      <td>825</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0    0   1   2   3    4   5\n",
       "row_0                          \n",
       "0      144  12  61  61  803  71\n",
       "1      169   7  48  75  840  79\n",
       "2      163   5  74  54  859  77\n",
       "3      134   7  64  62  846  86\n",
       "4      154   4  62  75  818  95\n",
       "5      165   9  49  56  825  84"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guessed_class = np.random.randint(0,6, y.shape[0])\n",
    "m=pd.crosstab(guessed_class, true_labels)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy of randomly guessing classes: 0.17.\n"
     ]
    }
   ],
   "source": [
    "print('Classification accuracy of randomly guessing classes: {:.2f}.'.format(np.trace(m.to_numpy()) / len(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Learning rate\n",
    "Learning rate is a **hyper-parameter** that controls how much we are adjusting the weights of our network with respect the loss gradient. \n",
    "Lower LR takes more time but allows better allocation of local minimum, higher LR allows faster calculations but drastic jumps do not always yield good results. \n",
    "        $$ newWeights=OldWeights - learningRate *gradientOfOldWeights$$ \n",
    "\n",
    "<img src=\"http://cs231n.github.io/assets/nn3/learningrates.jpeg\" alt=\"Drcng\" style=\"width: 400px;\"/>Img Credit: cs231n\n",
    "\n",
    "One can improve the results of computations significantly if learning rate is set well. However, learning rate might not remain the same throughout the training. The concpet of cyclical learning rate was introduced by Leslie N.Smith in 2015, it conveys a certain schedule when the LR starts with small values and increases (either linearly or exponentially) at each iteration. Learning rate decay would provide an alternative, it would bare the same problem though - when to decay the LR (step decay, exponential decay, others). In practice, step decay is preferred by many practitioners as hyperparameters it involves (the fraction of decay and the step timings in units of epochs) are more interpretable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning_rate = 0.1\n",
    "decay_rate = learning_rate / epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Momentum\n",
    "\n",
    "$$\\Delta  W_{i} = -learningRate  \\frac{\\partial L}{\\partial W} + \\mu  \\Delta W_{i-1}$$\n",
    "\n",
    "\n",
    "Second part that contains $\\mu$ is a momentum term here (or coefficient), that defines the effect of the accumulated past gradient (we are aking an exponentially weighted moving average of accumulated gradient). You can think of it as a certain velocity control mechanism. When we reach flatter areas, it will increase the speed of convergence, while dampening oscillations when reaching high curvatures. If learning rate measures how much the current situation affects the next step, while momentum measures how much past steps affect the next step. \n",
    "\n",
    "Conventional values to set for momentum is 0.5 increasing to 0.9, in case of cross validation can be set to values such as [0.5, 0.9, 0.95, 0.99]\n",
    "\n",
    "**Nesterov Momentum** is a slightly different version of the momentum update that has recently been gaining popularity. It is set as a metaparameter in basic Keras application that we will see in the next tutorial. In simplified terms, Nesterov momentum gives gradient a better 'nudge' as it containes a 'lookahead' information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further study\n",
    "\n",
    "I strongly recommend watching this video from 3blue1brown on neural network training using gradient descent and backpropagation: https://www.youtube.com/watch?v=tIeHLnjs5U8&list=PLLMP7TazTxHrgVk7w1EKpLBIDoC50QrPS&index=4\n",
    "\n",
    "In my opinion, this is the single most intuitive explanation available on the internet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (adams)",
   "language": "python",
   "name": "pycharm-feb95198"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
