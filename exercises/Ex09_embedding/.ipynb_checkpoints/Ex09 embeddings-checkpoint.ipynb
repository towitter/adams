{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello text #\n",
    "Written/spoken text is unstructured data. Teaching the machines to \"understand\" our language has been one of the hardest tasks researchers from computer science and linguistic fields had to face. It is still work in progress but the modern methods of **natural language processing** offer very impressive results. We need to map the semantic diversity that our brain operates in to the numeric field that copmuter can \"understand\". Thus our goal is to represent a meaning of the word/sentence/text with a real-number vector. How can we do that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Dealing with unstructured data might seem intimidating at first, but fear not, we will explore several methods to prepare your text for whatever methods you are planning to apply. We will look into:\n",
    " - tokenizing\n",
    " - removing punctuations\n",
    " - removing stop words, sparse terms, and particular words\n",
    " - stemming/lemmatizing\n",
    " \n",
    "For all that we will use a really nice package called **nltk*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download() #- for some extra untilities\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Stefan\n",
      "[nltk_data]     Lessmann\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "text = \"I wonder if I have been changed in the night. Let me think. Was I the same when I got up this morning? I almost can remember feeling a little different. But if I am not the same, the next question is 'Who in the world am I?' Ah, that is the great puzzle!\"\n",
    "#hope no need to quote :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step: **tokenize** it. You can make tokens to be sentences or characters, we will go for words (word tokenization). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'wonder', 'if', 'I', 'have', 'been', 'changed', 'in', 'the', 'night', '.', 'Let', 'me', 'think', '.', 'Was', 'I', 'the', 'same', 'when', 'I', 'got', 'up', 'this', 'morning', '?', 'I', 'almost', 'can', 'remember', 'feeling', 'a', 'little', 'different', '.', 'But', 'if', 'I', 'am', 'not', 'the', 'same', ',', 'the', 'next', 'question', 'is', \"'Who\", 'in', 'the', 'world', 'am', 'I', '?', \"'\", 'Ah', ',', 'that', 'is', 'the', 'great', 'puzzle', '!']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokens = [nltk.word_tokenize(item) for item in text] in case you have a list of phrases\n",
    "tokens=nltk.word_tokenize(text)\n",
    "print(tokens)\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm, but the punctuation is there still. Is it noise or is it useful? Let's try removing it for now (there is a bunch of methods out there). Additionally we will drop weird symbols and lower the big cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'wonder', 'if', 'i', 'have', 'been', 'changed', 'in', 'the', 'night', 'let', 'me', 'think', 'was', 'i', 'the', 'same', 'when', 'i', 'got', 'up', 'this', 'morning', 'i', 'almost', 'can', 'remember', 'feeling', 'a', 'little', 'different', 'but', 'if', 'i', 'am', 'not', 'the', 'same', 'the', 'next', 'question', 'is', 'in', 'the', 'world', 'am', 'i', 'ah', 'that', 'is', 'the', 'great', 'puzzle']\n"
     ]
    }
   ],
   "source": [
    "tokens2 = [word.lower() for word in tokens if word.isalpha()] #alpha -> alphanumeric characters\n",
    "\n",
    "print(tokens2)\n",
    "\n",
    "#alternative method\n",
    "#from nltk.tokenize import RegexpTokenizer\n",
    "#tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is starting to look like a dictionary already, right? There are some more issues we want to address though. Like 'stop words' - semantically they do not mean much but serve to put sentences together (\"the\", \"a\", \"and\", etc) - they will add noise. NLTK can offer you its own list of stop words, or you can decide on them yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wonder', 'changed', 'night', 'let', 'think', 'got', 'morning', 'almost', 'remember', 'feeling', 'little', 'different', 'next', 'question', 'world', 'ah', 'great', 'puzzle']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")\n",
    "#stop_words.append(“some_word_you_dont_like”)\n",
    "tokens3 = [word for word in tokens2 if word not in stop_words]\n",
    "print(tokens3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have already thought of the issue: what if the word is same but in different form? It will be treated as different words semantically right? That is where **lemmatizing** comes in. It's more sophisticated than stemming that just drops the suffixes, but it will need to look up the dictionary, that we will load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wonder', 'change', 'night', 'let', 'think', 'get', 'morning', 'almost', 'remember', 'feel', 'little', 'different', 'next', 'question', 'world', 'ah', 'great', 'puzzle']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens4 = [lemmatizer.lemmatize(word, pos='n') for word in tokens3]\n",
    "tokens4 = [lemmatizer.lemmatize(word, pos='v') for word in tokens4]\n",
    "print(tokens4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strip\n",
      "stripe\n"
     ]
    }
   ],
   "source": [
    "#it also takes a little longer than stemming and you need to choose the type of word:\n",
    "print(lemmatizer.lemmatize(\"stripes\", 'v'))  \n",
    "print(lemmatizer.lemmatize(\"stripes\", 'n'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wonder',\n",
       " 'chang',\n",
       " 'night',\n",
       " 'let',\n",
       " 'think',\n",
       " 'got',\n",
       " 'morn',\n",
       " 'almost',\n",
       " 'rememb',\n",
       " 'feel',\n",
       " 'littl',\n",
       " 'differ',\n",
       " 'next',\n",
       " 'question',\n",
       " 'world',\n",
       " 'ah',\n",
       " 'great',\n",
       " 'puzzl']"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer= PorterStemmer()\n",
    "tokens4=[stemmer.stem(word)for word in tokens3]\n",
    "tokens4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding\n",
    "Finally let's set up our one-hot encoding matrix. First we need to load some tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wonder' 'chang' 'night' 'let' 'think' 'got' 'morn' 'almost' 'rememb'\n",
      " 'feel' 'littl' 'differ' 'next' 'question' 'world' 'ah' 'great' 'puzzl']\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Convert tokens to a numpy array\n",
    "tokens5 = np.array(tokens4)\n",
    "print(tokens5)\n",
    "print(len(tokens5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can start with the actual encoding, which happends in two steps. First, we map our tokens to a numeric presentation using LabelEncoder. A token will be convert to its index in an ordered list of tokens. Check it out below. The first element in the encoded array is 16.\n",
    "Does that make sense? Where do you see a value of zero? Plausible?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16  2 11  7 15  5  9  1 14  4  8  3 10 13 17  0  6 12]\n",
      "(18,)\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "encoded = label_encoder.fit_transform(tokens5)\n",
    "print(encoded)\n",
    "print(encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is data from which we can create our one-hot-encoded matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "(18, 18)\n"
     ]
    }
   ],
   "source": [
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "encoded = encoded.reshape(len(encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(encoded)\n",
    "print(onehot_encoded)\n",
    "print(onehot_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "['got']\n",
      "['think' 'got' 'morn']\n"
     ]
    }
   ],
   "source": [
    "#Let's check 6th word\n",
    "inverted = label_encoder.inverse_transform([np.argmax(onehot_encoded[5, :])])\n",
    "print(onehot_encoded[5, :])\n",
    "print(inverted)\n",
    "print(tokens5[4:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM example with one-hot encoding\n",
    "Let's try using an LSTM classifier to predict the next word in the sentence. Specifically, we will feed the network a \n",
    "of sequence of words of length _timesteps_ and ask it to predict the next word following that sequence.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "timesteps = 3 # we will feed in *timesteps* words to predict the next word\n",
    "\n",
    "\n",
    "def create_dataset(time_series, timesteps):\n",
    "    \"\"\" Helper function to transform an input sequence into data for supervised learning  \"\"\" \n",
    "    dataX, dataY = [], []\n",
    "    \n",
    "    for i in range(0,len(time_series) - timesteps):\n",
    "        x = time_series[i:i + timesteps]           \n",
    "        dataX.append(x)\n",
    "        y = time_series[i + timesteps] \n",
    "        dataY.append(y)\n",
    "           \n",
    "    return np.array(dataX), np.array(dataY) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 3, 18)\n",
      "(15, 18)\n"
     ]
    }
   ],
   "source": [
    "timesteps = 3\n",
    "\n",
    "X, y = create_dataset(onehot_encoded, timesteps)#lookback\n",
    "print(X.shape) # nb obs, nb timestep, nb features=18, which is pretty much the size of our vocab\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0.]])"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[2, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0.])"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun a little verification that everything we do with our vectors and matrices makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['night']\n",
      "['let']\n",
      "['think']\n",
      "is followd by next word \n",
      "['got']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['wonder', 'chang', 'night', 'let', 'think', 'got', 'morn',\n",
       "       'almost', 'rememb', 'feel', 'littl', 'differ', 'next', 'question',\n",
       "       'world', 'ah', 'great', 'puzzl'], dtype='<U8')"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for w in X[2, :, :]:\n",
    "    print(label_encoder.inverse_transform([np.argmax(w)]))\n",
    "print('is followd by next word ')\n",
    "print(label_encoder.inverse_transform([np.argmax(y[2])]))\n",
    "tokens5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is hard to think of text as vectors/matrices. You will soon get used to it. Let's now continue with our LSTM classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 2.9024 - accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 2.8940 - accuracy: 0.0000e+00\n",
      "Epoch 3/20\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 2.8885 - accuracy: 0.0000e+00\n",
      "Epoch 4/20\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 2.8828 - accuracy: 0.0000e+00\n",
      "Epoch 5/20\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 2.8771 - accuracy: 0.0667\n",
      "Epoch 6/20\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 2.8716 - accuracy: 0.0667\n",
      "Epoch 7/20\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 2.8665 - accuracy: 0.0667\n",
      "Epoch 8/20\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 2.8607 - accuracy: 0.0667\n",
      "Epoch 9/20\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 2.8554 - accuracy: 0.1333\n",
      "Epoch 10/20\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 2.8501 - accuracy: 0.1333\n",
      "Epoch 11/20\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 2.8445 - accuracy: 0.1333\n",
      "Epoch 12/20\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 2.8390 - accuracy: 0.1333\n",
      "Epoch 13/20\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 2.8335 - accuracy: 0.1333\n",
      "Epoch 14/20\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 2.8280 - accuracy: 0.1333\n",
      "Epoch 15/20\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 2.8220 - accuracy: 0.2000\n",
      "Epoch 16/20\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 2.8160 - accuracy: 0.2000\n",
      "Epoch 17/20\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 2.8102 - accuracy: 0.2000\n",
      "Epoch 18/20\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 2.8041 - accuracy: 0.2000\n",
      "Epoch 19/20\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 2.7971 - accuracy: 0.2000\n",
      "Epoch 20/20\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 2.7903 - accuracy: 0.2667\n",
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_20 (LSTM)               (None, 5)                 480       \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 18)                108       \n",
      "=================================================================\n",
      "Total params: 588\n",
      "Trainable params: 588\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nb_hidden = 5\n",
    "model = Sequential()\n",
    "model.add(LSTM(nb_hidden, input_shape=(timesteps, X.shape[2])))\n",
    "model.add(Dense(18, activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics=['accuracy'])\n",
    "\n",
    "story=model.fit(X, y, batch_size=1, epochs=20, shuffle=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy does not look good. Obviously far too few observations. Let's look at some predictions of our network.\n",
    "Remember that our 'sentence' or rather token sequence was: <br>\n",
    "'wonder', 'chang', 'night', 'let', 'think', 'got', 'morn', 'almost', 'rememb', 'feel', 'littl', 'differ', 'next', 'question', 'world', 'ah', 'great', 'puzzl'\n",
    "<br>\n",
    "We take the last three tokens of the sequence and see what our LSTM has predicted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target 'ah' predictions with decreasing prob: 'littl', 'got', 'ah'\n",
      "Target 'great' predictions with decreasing prob: 'almost', 'great', 'let'\n",
      "Target 'puzzl' predictions with decreasing prob: 'puzzl', 'differ', 'morn'\n"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "yhat = model.predict(X[-n:])\n",
    "# predicted density across the words in our vocab \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(yhat[0])\n",
    "plt.show();\n",
    "# most probable words\n",
    "ix = np.argsort(yhat, axis=1)\n",
    "\n",
    "demo = {'y': label_encoder.inverse_transform(np.argmax(y[-n:,], axis=1)),\n",
    "        'yhat': np.empty((n,3), dtype=object)}\n",
    "\n",
    "for i in range(3):\n",
    "    top3 = label_encoder.inverse_transform(ix[i,-3:])\n",
    "    demo['yhat'][i,:] = top3\n",
    "    print(\"Target '{}' predictions with decreasing prob: '{}', '{}', '{}'\".format(\n",
    "        demo['y'][i], demo['yhat'][i,-1], demo['yhat'][i,-2], demo['yhat'][i,-3]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Ok, at least the model got it almost right occasionally.  \n",
    "Let's continue with taking a closer look into the LSTM and its layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_20 (LSTM)               (None, 5)                 480       \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 18)                108       \n",
      "=================================================================\n",
      "Total params: 588\n",
      "Trainable params: 588\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Here is the model architecture again. So we are interested in the weights of layer 0\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(18, 20) (5, 20) (20,)\n"
     ]
    }
   ],
   "source": [
    "embs = model.layers[0].get_weights()\n",
    "print(len(embs))\n",
    "print(embs[0].shape, embs[1].shape, embs[2].shape )\n",
    "\n",
    "#numpy array of weights for inputs, numpy array of weights for hidden units, a numpy array of bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the following equations from our LSTM session in the lecture. They remind us about the processing\n",
    "of our input data, which has 18 features in this example.\n",
    "$$ I_t = \\sigma \\left( X_t W_{xi} + H_{t-1}W_{hi} + b_i \\right) $$\n",
    "$$ F_t = \\sigma \\left( X_t W_{xf} + H_{t-1}W_{hf} + b_f \\right) $$\n",
    "$$ O_t = \\sigma \\left( X_t W_{xo} + H_{t-1}W_{ho} + b_o \\right) $$\n",
    "$$ \\tilde{C}_t = tanH \\left( X_tW_{xc} + H_{t-1}W_{hc} + b_c\\right) $$\n",
    "\n",
    "Keras saves the weights for the different gates in one matrix. This explains the dimensions we observed above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 20)"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs[0].shape # features x nb_hidden units * 4 for W_{xi}, W_{xf}, W_{xo}, W_{xi}\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md \n"
    }
   },
   "source": [
    "We'll use an uninteresting helper function to extract the input matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#https://fairyonice.github.io/Extract-weights-from-Keras's-LSTM-and-calcualte-hidden-and-cell-states.html\n",
    "def get_LSTMweights(model1):\n",
    "    for layer in model1.layers:\n",
    "        if \"LSTM\" in str(layer):\n",
    "            w = layer.get_weights()\n",
    "            W,U,b = get_LSTM_UWb(w)\n",
    "            break\n",
    "    return W\n",
    "\n",
    "def get_LSTM_UWb(weight):\n",
    "    '''\n",
    "    weight must be output of LSTM's layer.get_weights()\n",
    "    W: weights for input\n",
    "    U: weights for hidden states\n",
    "    b: bias\n",
    "    '''\n",
    "    warr,uarr, barr = weight\n",
    "    gates = [\"i\",\"f\",\"c\",\"o\"]\n",
    "    hunit = uarr.shape[0] # dim. of hidden state\n",
    "    U, W, b = {},{},{}\n",
    "    for i1,i2 in enumerate( range(0, len(barr), hunit) ): #range(start, stop, step)\n",
    "        \n",
    "        W[gates[i1]] = warr[:,i2:i2+hunit]\n",
    "        U[gates[i1]] = uarr[:,i2:i2+hunit]\n",
    "        b[gates[i1]] = barr[i2:i2+hunit].reshape(hunit,1)\n",
    "    return(W,U,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input gate: 18x5\n"
     ]
    }
   ],
   "source": [
    "em=get_LSTMweights(model)\n",
    "print('Input gate: ' + 'x'.join(map(str,em[\"i\"].shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that the input layer is acting as an embedding layer already. \n",
    "To see this, we'll follow the process from word to LSTM input step-by-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.33618477,  0.34431243,  0.4250325 ,  0.03895715, -0.41110694],\n",
       "       [ 0.32641658,  0.13220686, -0.02276477,  0.03564529, -0.17644253],\n",
       "       [-0.00415924,  0.10697288,  0.1505544 ,  0.2791538 , -0.20221394],\n",
       "       [-0.11949009,  0.23577814,  0.28123912,  0.15469721,  0.18655679],\n",
       "       [ 0.33962283,  0.1054621 ,  0.15804794,  0.29487282, -0.3251031 ],\n",
       "       [-0.07039982, -0.2768701 , -0.13023408,  0.22185716, -0.22999388],\n",
       "       [ 0.33313358,  0.18024035, -0.1297541 ,  0.09564445, -0.26427785],\n",
       "       [ 0.04485145, -0.36427078,  0.0783466 ,  0.09424914,  0.5583681 ],\n",
       "       [-0.09801802,  0.21367215, -0.02617562,  0.23904914, -0.09910572],\n",
       "       [ 0.25140005,  0.0939674 , -0.1884339 ,  0.06482703,  0.34196898],\n",
       "       [-0.40521583,  0.18694869,  0.20639311,  0.04477953, -0.2860861 ],\n",
       "       [ 0.14880055, -0.3122058 , -0.20310748, -0.17195523,  0.38780025],\n",
       "       [-0.20881966, -0.07266429, -0.18655178, -0.12024415,  0.1425313 ],\n",
       "       [-0.01871759, -0.2134449 , -0.32939088,  0.04117384, -0.27885318],\n",
       "       [-0.00941493,  0.23961356, -0.1693973 ,  0.5478339 , -0.28437737],\n",
       "       [-0.18913923,  0.16634901, -0.14534336, -0.11299708, -0.13855407],\n",
       "       [ 0.12441248, -0.11429241, -0.00520559, -0.06106023, -0.10490729],\n",
       "       [-0.13758045,  0.13049157, -0.06369375,  0.25081703, -0.09898536]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em[\"i\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15], dtype=int64)"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"think\"\n",
    "word_label = label_encoder.transform([word])\n",
    "word_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "(1, 18)\n"
     ]
    }
   ],
   "source": [
    "word_dummy = onehot_encoder.transform(word_label.reshape(1,-1))\n",
    "print(word_dummy)\n",
    "print(word_dummy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input layer multiplies the one-hot encoded input vector with a Dense weight matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the embedding for the word think.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.18913923,  0.16634901, -0.14534336, -0.11299708, -0.13855407]])"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e=word_dummy.dot(em[\"i\"])\n",
    "print(f\"Here is the embedding for the word {word}.\")\n",
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what's that talk about Embedding layers? We could speed up the process of looking up a row by literally looking up the row in the table!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.18913923,  0.16634901, -0.14534336, -0.11299708, -0.13855407]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em[\"i\"][word_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This speedup, and some convenient functionality, make using Embedding layers worthwhile. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want to end up with is a __word vector/word embedding__, looking like this:\n",
    "\n",
    "$$air=  \\begin{bmatrix} -0.3\\\\ 0.001\\\\   2\\\\-1.1\\\\0.07\\\\ \\end{bmatrix}$$\n",
    "\n",
    "Here we have encapsulated the meaning of the word **air** within the 5x1 vector/embedding - we now have a *distributed representation*. The number of levels may be changed, depending on the necessary level of \"detail\".\n",
    "\n",
    "The big benefit of such vectors is one can operate with them on the same subspace and expect a meaningful result:\n",
    "\n",
    "![emb](https://humboldt-wi.github.io/blog/img/seminar/topic_models/vectors.png)\n",
    "\n",
    "The downside is that the coordinates represent certain latent semantic features of embeddings and thus can not be surely interpreted.\n",
    "\n",
    "https://ronxin.github.io/wevi/ - for demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training embeddings\n",
    "Remember Autoencoders? We were giving the network a \"fake\" task to compress an image, after the training we would drop the decoder part and only use the encoder weight matrix as the \"compression tool\". The task with embeddings will be very similar. We give the NN a \"fake\" task: given a specific word in the middle of a chosen window (the input word), look at the words nearby and pick one at random. The network should tell us the probability for every word in our vocabulary of being the “nearby word” that we chose. The probability distribution then gets compared with the 'true' sentence, networks gets a penalty (loss function value) and with the familiar technique of backpropagation it improves the weights. \n",
    "\n",
    "How will we know it's good? Well, if we ask what is the next most probable word after \"fast\", the \"car\" or \"track\" should have higher probabilities than \"chopsticks\".\n",
    "\n",
    "Let's break it down with a famous example phrase \"The quick brown fox jumps over the lazy dog\".\n",
    "It gives us a dictionary of 8 words. We consider a window of size 2. If we need every word to be a center word at least once, how would the training sample look like then?\n",
    "\n",
    "![fox](http://mccormickml.com/assets/word2vec/training_data.png)\n",
    "http://mccormickml.com/assets/word2vec/\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with word \"brown\" (question for the class - will the order matter?)\n",
    "![structure](http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png\")\n",
    "\n",
    " 1. We generate one hot input vector with \"brown\" marked as 1\n",
    " \n",
    " [0 0 1 0 0 0 0 0]\n",
    " \n",
    " 2. Let's say, our hidden layer $h$ has 30 neurons (it's a hyperparameter that one can choose). We initialize the first weight matrix of size 30x8 as we did before (let's say random uniform). After training it will turn into an awesome embeddings look up table. The hidden layer will have no activation function, so let's just do matrix multiplication: the result will be the embeddings string that corresponds to the supplied word:\n",
    " ![structure](http://mccormickml.com/assets/word2vec/matrix_mult_w_one_hot.png)\n",
    " \n",
    " 3. The second weight matrix $W^{(2)}$ will also be randomly initialized in the beginning and have dimensions 8x30, we proceed with multiplication to get a score vector, that will have the size of our vocabulary times the size of the window (mind that you might think it should be window-1 so that only words around are contained, but the model trains the input word as well, every word gets embeddings that are averaged over its \"role\" as context and as target) :\n",
    " $$z= W^{(2)\\top} h$$\n",
    " \n",
    " 4. We plug the score vector $z$ into a softmax to get the probabilites distribution:\n",
    " $$\\hat{y}=softmax(z)$$\n",
    " \n",
    " We will get the probailities of observing every context word given our input. We want this probability vector to match the true vector that contains the words in the context.\n",
    " \n",
    " \n",
    "Just like before, the loss will be calculated with a cross entropy and used for backpropagation.\n",
    "\n",
    "The exsiting part is that in case two words repeat in the same vicinity/in the same context - their embeddings will be similair - like \"tea\" and \"coffee\" will both be noticed in the context of \"drink\", \"hot\", \"cup\", and \"caffeine\". That is exactly what we were going for!\n",
    "\n",
    " \n",
    "http://mccormickml.com/assets/word2vec/Alex_Minnaar_Word2Vec_Tutorial_Part_I_The_Skip-Gram_Model.pdf\n",
    "! Check the dimension of the output layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras implementation\n",
    "\n",
    "Let's try to do an example task of text classification. We will get an evaluation form on this course, let's say we know how every student commmented on the course and wether the student took the final exam/assignment or not. Some time in the future we will want to predict if the student finished with the course given the new comment. But for now we just want to get the embeddings.\n",
    "\n",
    "Let's start with trying to train our own embeddings. Keras offers an [embeddings layer](https://keras.io/layers/embeddings/#embedding) for the implementation of the technique we described above. This layer may be used in several ways:\n",
    " - to learn a word embedding that can be saved and used in another model later (or together with the model)\n",
    " - to load a pre-trained word embedding model, a type of transfer learning (this option is the most wide-spread implementation nowadays - in order to save the computational resources several pretrained embeddings are available for download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "#We will be inspiredbyt his nice tutorial https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#Example\n",
    "model = Sequential()\n",
    "#acts as the first hidden layer\n",
    "model.add(Embedding(1000, output_dim=64)) # first input is the size of vocabulary, second - the size of hidden layer, third - length of input sequence or number of words (window size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 64)          64000     \n",
      "=================================================================\n",
      "Total params: 64,000\n",
      "Trainable params: 64,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the embeddings are concatenated at any point in the architecture, then the size of subsequent layers depends on the number of inputs. To define the model, we'll need to tell keras our expectation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(1000, output_dim=64, input_length=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 10, 64)            64000     \n",
      "=================================================================\n",
      "Total params: 64,000\n",
      "Trainable params: 64,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will take as input an integer matrix of size (batch, input_length). Now `model.output_shape == (None, 10, 64)`, where `None` is the batch dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text and vocabulary\n",
    "We won't do preprocessing here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# student reviews\n",
    "reviews = ['pretty good','very nice','god, never again','better than expected','loved the course','it was too hard','tutorials were nice','not so good','did not work for me','good course.']\n",
    "# student took the final assignment =1, didn't take the assignment =0\n",
    "labels = np.array([1,1,0,1,1,0,1,0,0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you remember, machines can't really deal with strings, so we will need to find a way to transform every word into a hashed integer, basically getting an indexed vocabulary (pretty=0512, good=14772, etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16, 16], [3, 8], [34, 10, 43], [49, 6, 42], [16, 7, 12], [27, 24, 46, 33], [33, 39, 8], [32, 48, 16], [8, 32, 15, 19, 9], [16, 12]]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50 #number you can choose, just make sure it's bigger than the actual word count\n",
    "encoded = [one_hot(single_review, vocab_size) for single_review in reviews]\n",
    "print(encoded)\n",
    "# mind that it ignores the punctuation. If some word is misspelled - it will be treated as a new word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But seems that our sequences are of different length and we have to supply embeddings layer with 1 number. We will need *paddings*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16 16  0  0  0]\n",
      " [ 3  8  0  0  0]\n",
      " [34 10 43  0  0]\n",
      " [49  6 42  0  0]\n",
      " [16  7 12  0  0]\n",
      " [27 24 46 33  0]\n",
      " [33 39  8  0  0]\n",
      " [32 48 16  0  0]\n",
      " [ 8 32 15 19  9]\n",
      " [16 12  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# seems that longest one contains 5 words\n",
    "max_length = 5 # that is our window size\n",
    "padded= pad_sequences(encoded, maxlen=max_length, padding='post')\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it once with a concatenation and a Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 5, 10)             500       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 551\n",
      "Trainable params: 551\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=max_length)) #10 chosen arbitrarily, can not be too high in our case - too little data\n",
    "model.add(Flatten()) #Embedding layer will be 5 vectors of 10 dimensions each, one for each word. Our output however is the size of window x the size of vocabulary, so\n",
    "#we have to flatten it to 50 elements vector to pass on to the Dense output layer.\n",
    "model.add(Dense(1, activation='sigmoid')) # we have a binary prediction\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "#Question for the class-  why 51 in the last layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's go back to the LSTM that we did at the beginning. This time, we'll add the embedding layer *before* the LSTM input layer to increase speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 10)          500       \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 15)                1560      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 16        \n",
      "=================================================================\n",
      "Total params: 2,076\n",
      "Trainable params: 2,076\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10)) #10 chosen arbitrarily, can not be too high in our case - too little data\n",
    "model.add(LSTM(15))\n",
    "model.add(Dense(1, activation='sigmoid')) # we have a binary prediction\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "#Question for the class-  why 51 in the last layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\adams\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 [==============================] - 0s 31ms/step - loss: 0.6932 - acc: 0.4000\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 0s 402us/step - loss: 0.6922 - acc: 0.6000\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 0s 297us/step - loss: 0.6911 - acc: 0.7000\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 0s 299us/step - loss: 0.6902 - acc: 0.7000\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 0s 195us/step - loss: 0.6892 - acc: 0.6000\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 0s 298us/step - loss: 0.6882 - acc: 0.6000\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 0s 399us/step - loss: 0.6871 - acc: 0.6000\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 0s 299us/step - loss: 0.6861 - acc: 0.6000\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 0s 398us/step - loss: 0.6850 - acc: 0.6000\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 0s 399us/step - loss: 0.6840 - acc: 0.6000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x20e11a28d48>"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(padded, labels, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 4ms/step\n",
      "Accuracy: 60.000002\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded, labels, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3, 6, 7, 8, 9, 10, 12, 15, 16, 19, 24, 27, 32, 33, 34, 39, 42, 43, 46, 48, 49}"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index of unique words\n",
    "word_idx = set([word for single_review in encoded for word in single_review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03365597,  0.00884666,  0.0427767 ,  0.01578626,  0.0076389 ,\n",
       "         0.00171707, -0.01439738, -0.0536136 ,  0.02153645, -0.0472912 ],\n",
       "       [-0.01846614, -0.03721134, -0.03149301, -0.0392504 ,  0.00789683,\n",
       "         0.00918467,  0.01901242,  0.02566249,  0.02917756, -0.04449408],\n",
       "       [-0.02354015,  0.01786951, -0.01034049,  0.0176363 ,  0.0118949 ,\n",
       "         0.03884137,  0.00204159,  0.04806424,  0.0267591 , -0.03961788],\n",
       "       [-0.03555783, -0.01210421, -0.0034983 , -0.02245114,  0.02582309,\n",
       "         0.02517932, -0.00094708,  0.03603847, -0.03908114, -0.00754421]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs=model.layers[0].get_weights()\n",
    "embs[0][:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do these numbers make sense? Let's check the correlation of good (4) to never (2) and nice (49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.32767901, 0.22969781])"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(embs[0], rowvar=True)[4,[2,49]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordNet\n",
    "A huge effort to create and maintain, can not provide context filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('mechanism.n.05'),\n",
       " Synset('device.n.01'),\n",
       " Synset('instrumentality.n.03'),\n",
       " Synset('artifact.n.01'),\n",
       " Synset('whole.n.02'),\n",
       " Synset('object.n.01'),\n",
       " Synset('physical_entity.n.01'),\n",
       " Synset('entity.n.01')]"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn \n",
    "robot = wn.synset(\"robot.n.01\")\n",
    "hyper = lambda s: s.hypernyms() # definitions - *** is a...\n",
    "list(robot.closure(hyper)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training on an unrelated task: Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we get those embeddings? ##\n",
    "In 2013 Mikolov et al. offered a revolutionary **word2vec** technique for learning word vectors:\n",
    " 1. get large corpus of text (preferably from the domain that you are interested in)\n",
    " 2. every word in this text will be part of the vocabulary and it will get its own vector\n",
    " 3. select a \"window\" size (we did - 5 words) and go through the text window by window, checking the center word (that would be 3rd word in every context window, like \"I have **been** changed in\", \"have been **changed** in the\", \"been changed **in** the night\"..etc)\n",
    " 4. calculate the probability of getting a certain context given a certain word or vise versa by comparing the word vectors/embeddings (if we have **changed**,- what is the probability of getting \"been\",\"in\", etc)\n",
    " 5. compare the predicted words/contexts to the true ones and keep adjusting the embeddings until you grasp the meaning well enough (maximize the probability of the correct choice, in our case of \"been\")\n",
    " \n",
    " N.B. Every word will get a weight a center word and as a context word. At the end both will be averaged.\n",
    " \n",
    " already sounds familiar, right?\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word’s meaning is given by the words that frequently appear close-by (distributional semantics).\n",
    "\n",
    "The context of a word is the words that appear nearby within a certain window size (let's say 5 words).\n",
    "\n",
    " - the quality of *air* in mainland China has been decreasing since..\n",
    " - doctors claim the *air* you breath defines the overall wellbeing...\n",
    " - the currents of hot *air* have been bursting from underground\n",
    " - the mountain *air* was crystal clean and filled with ..\n",
    " - in case of *air* supply shortages, the submarine will..\n",
    " \n",
    "Words around *air* will define the **meaning** of the word for our NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on models ###\n",
    "As said we can go two ways about it:\n",
    "\n",
    "1. Skip-grams (SG) - we will focus on it\n",
    "\n",
    "Predict context (”outside”) words  given center word\n",
    "\n",
    "\n",
    "2. Continuous Bag of Words (CBOW)\n",
    "\n",
    "Predict center word from (bag of) context words\n",
    "\n",
    "\n",
    "[? ? **in** ? ?]   VS   [been changed **?** the night]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How would our NN structure look like for this task (Skip-gram)###\n",
    "\n",
    " - input layer has length **V** - the size of our dictionary (how many words is contained in our text corpus), it will be a one-hot encoded vector that will cancel out every wordin the dictionary except for  the one we are passing as observation\n",
    " - we will need to train two weight matrices (**W** and W\" - looking ahead, it will be only the W matrix that will contain the word embeddings that we will need). \n",
    " - **N** is our hidden layer, it defines the size of every word embedding\n",
    " - **C** is the number of context words (in the past example it was 4)\n",
    " \n",
    "![sg](skip-gram.png)\n",
    "\n",
    "Source: https://upload.wikimedia.org/wikipedia/commons/9/95/Skip-gram.png\n",
    "\n",
    "Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). Efficient estimation of word representations in vector space. CoRR, abs/1301.3781."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another bit that is missing here is the training mechanism - how will we know if the chosen probability is good or bad? We are back to our multiclass problem basically, so we will need good old softmax and cross-entropy loss function (with the amendment that for efficiency we will need hierarchich softmax and negative sampling)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (adams)",
   "language": "python",
   "name": "pycharm-feb95198"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
